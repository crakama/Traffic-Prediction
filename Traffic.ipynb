{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Traffic Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The project investigates road traffic problem of detecting anomalous behaviour using data recorded by sensors. The model uses density and its density-difference features lean a pattern from the data,if the density spikes beyond a certain threshold then anomaly is detected.\n",
    "\n",
    "The model is intended to be implemented in a solution such as a mobile application to be used by drivers and motorists to know in advance about road status hence make good decisions for better travelling experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and Algorithms\n",
    "\n",
    "The solution will be implemented using unsupervised learning algorithm, auto-encoder neural network, to develop models that will then be used to predict anomalous observations of traffic flow data.  Anomalous observations are irregularities that cause a change in normal flow of traffic. These include; natural disaster, accident, public activities such as marathon or road construction. \n",
    "\n",
    "Unsupervised learning algorithm, autoencoder has been used together with Long Short Term Memory(LSTM). The choice behind this design decision is because the problem is categorized as a sequence classification problem, since data is read from sensors as a sequence of sensor readings, based on the reading, the model can classify if there is usual or unusual behaviour at a certain time. Recurrent Neural Networks (RNNs) are known for solving such problems. RNNs contains loops of a basic neural network where activations or output from previous time step act as input to the next time step, in addition to a new data sample and together they are used to make new predictions.\n",
    "\n",
    "Autoencoder, a neural network algorithm is used to help reconstruct data. In many cases, Neural networks are given inputs and expected to predict a target. In this project, un-labeled dataset with no traffic problems will be used to train the neural network, then, the network re-constructs the data input as its prediction. The network will then be tested with un-censored traffic data which contains both normal and abnormal trends. The model is expected to fail to re-construct the new data by showing a spike in prediction error, hence detection of traffic.LSTM is used to analyse time series sensor data.\n",
    "\n",
    "### Dataset\n",
    "Public dataset on traffic flow from Hops platform was used. TrafficFlow_Sample (hdfs:///Projects/TrafficFlow/TrafficFlow_Sample/) -  The dataset is a sample of traffic flow data collected from sensors deployed on Stockholm highways. The dataset is labelled to indicate good and bad traffic conditions. The labeled portion will be used to filter data with good traffic conditions which is used to train the model using a density feature engineered based on other existing feature fro the dataset i.e Flow-In, Timestamp and Average-Speed.\n",
    "\n",
    "### Tools and platform used\n",
    "Keras to implement autoencoder and LSTM deep learning algorithm\n",
    "Apache Spark Cluster running on Cognitive AI platform (https://labs.cognitiveclass.ai/), Jupyter notebook, Pyspark\n",
    "\n",
    "\n",
    "# Steps: \n",
    "\n",
    "## 1. Import and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf , col, lag, datediff, unix_timestamp,lit,coalesce,concat,split, explode\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_flow = StructType().add('Timestamp', TimestampType(), False) \\\n",
    "        .add('Ds_Reference', StringType(), False) \\\n",
    "        .add('Detector_Number', ShortType(), False) \\\n",
    "        .add('Traffic_Direction', ShortType(), False) \\\n",
    "        .add('Flow_In', ShortType(), False) \\\n",
    "        .add('Average_Speed', ShortType(), False) \\\n",
    "        .add('Sign_Aid_Det_Comms', ShortType(), False) \\\n",
    "        .add('Status', ShortType(), False) \\\n",
    "        .add('Legend_Group', ShortType(), False) \\\n",
    "        .add('Legend_Sign', ShortType(), False) \\\n",
    "        .add('Legend_SubSign', ShortType(), False) \\\n",
    "        .add('Protocol_Version', StringType(), False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Ds_Reference: string (nullable = true)\n",
      " |-- Detector_Number: short (nullable = true)\n",
      " |-- Traffic_Direction: short (nullable = true)\n",
      " |-- Flow_In: short (nullable = true)\n",
      " |-- Average_Speed: short (nullable = true)\n",
      " |-- Sign_Aid_Det_Comms: short (nullable = true)\n",
      " |-- Status: short (nullable = true)\n",
      " |-- Legend_Group: short (nullable = true)\n",
      " |-- Legend_Sign: short (nullable = true)\n",
      " |-- Legend_SubSign: short (nullable = true)\n",
      " |-- Protocol_Version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv('data/mcs_201606.csv', sep=';', schema=schema_flow, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS')\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 56.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85255773"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "|          Timestamp|Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "|2016-06-01 00:00:00| E182N 2,015|             49|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             50|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             51|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             52|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,325|             49|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "|Detector_Number|          Timestamp|Ds_Reference|Average_Speed|Flow_In|Status|\n",
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "|             49|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             52|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             49|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             49|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.select('Detector_Number','Timestamp','Ds_Reference','Average_Speed','Flow_In','Status').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Cleaning functions that generate sensor ID that will be used to uniquely identify a sensor, it consists of two fields from the dataset, Ds_Reference and Detector_Number number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See Reference [1]\n",
    "split_schema = StructType([\n",
    "  StructField('Road', StringType(), False),\n",
    "  StructField('Km_Ref', IntegerType(), False)\n",
    "])\n",
    "\n",
    "@udf(split_schema)\n",
    "def split_ds_ref(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        k, m = km.split(',')\n",
    "        meter = int(k)*1000 + int(m)\n",
    "        return r, meter\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "@udf(StringType())\n",
    "def split_ds_ref2(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        return r\n",
    "    except:\n",
    "        return None \n",
    "@udf(StringType())\n",
    "def split_ds_ref3(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        k, m = km.split(',')\n",
    "        meter = int(k)*1000 + int(m)  \n",
    "        return meter\n",
    "    except:\n",
    "        return None     \n",
    "#def generate_sensor_ids1(*cols):\n",
    "#    return concat(*[coalesce(c, lit(\"*\")) for c in cols]) \n",
    "def generate_sensor_ids(s, d):\n",
    "    r, km = s.split(' ')\n",
    "    k, m = km.split(',')\n",
    "    meter = int(k)*1000 + int(m)\n",
    "    var1 = r, meter\n",
    "    return var\n",
    "\n",
    "funcConcatCols = udf(lambda x,y,z: x+'_'+y+'_'+z,StringType())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up 1: Convert detector number to type int for easy manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_to_int = udf(lambda x : x - 48, ShortType())\n",
    "df_cleanup1 = df_raw.withColumn('Detector_Number', ascii_to_int('Detector_Number'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up 2 and 3: Split detector reference ID\n",
    "\n",
    "The field is first split up for easy concantination while generating unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|          Timestamp|Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|Ds_Ref_temp1|Ds_Ref_temp2|      Ds_Ref|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|2016-06-01 00:00:00| E182N 2,015|              1|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_1|\n",
      "|2016-06-01 00:00:00| E182N 2,015|              2|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_2|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+-------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|          Timestamp| Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|Ds_Ref_temp1|Ds_Ref_temp2|      Ds_Ref|\n",
      "+-------------------+-------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|2016-06-01 00:00:00|[E182N, 2015]|              1|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_1|\n",
      "|2016-06-01 00:00:00|[E182N, 2015]|              2|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_2|\n",
      "+-------------------+-------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleanup2 = df_cleanup1.withColumn('Ds_Ref_temp1',   split_ds_ref2('Ds_Reference')).withColumn('Ds_Ref_temp2',split_ds_ref3('Ds_Reference'))\n",
    "\n",
    "df_cleanup3 = df_cleanup2.withColumn('Ds_Ref', funcConcatCols(col('Ds_Ref_temp1'), col('Ds_Ref_temp2'),col('Detector_Number').cast(StringType())))\n",
    "df_cleanup3.show(2)\n",
    "\n",
    "df_cleanup4 = df_cleanup3.withColumn('Ds_Reference',split_ds_ref('Ds_Reference'))\n",
    "df_cleanup4.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up 4: Convert data to parquet format \n",
    "\n",
    "For faster execution of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup4.write.save('data/trafficData_E4N.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only normal data from the dataset, that is where status field indicates 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 951 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_trafficData_E4N = spark.read.parquet('data/trafficData_E4N.parquet').select('Timestamp', 'Ds_Reference', 'Ds_Ref', 'Detector_Number', 'Flow_In', 'Average_Speed').where('Status == 3 AND Ds_Reference.Road == \"E4N\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trafficData_E4N.createOrReplaceTempView(\"NormalTrafficFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+---------------+-------+-------------+\n",
      "|          Timestamp|Ds_Reference|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+\n",
      "|2016-06-09 01:45:00|[E4N, 47465]|E4N_47465_2|              2|      4|          101|\n",
      "|2016-06-09 01:45:00|[E4N, 47465]|E4N_47465_3|              3|      7|           84|\n",
      "|2016-06-09 01:45:00|[E4N, 47800]|E4N_47800_2|              2|      4|           98|\n",
      "|2016-06-09 01:45:00|[E4N, 47800]|E4N_47800_3|              3|      7|           91|\n",
      "|2016-06-09 01:45:00|[E4N, 48290]|E4N_48290_2|              2|      2|          104|\n",
      "|2016-06-09 01:45:00|[E4N, 48290]|E4N_48290_3|              3|      4|           97|\n",
      "|2016-06-09 01:45:00|[E4N, 48620]|E4N_48620_2|              2|      2|          119|\n",
      "|2016-06-09 01:45:00|[E4N, 48620]|E4N_48620_3|              3|      3|          105|\n",
      "|2016-06-09 01:45:00|[E4N, 48935]|E4N_48935_2|              2|      1|           96|\n",
      "|2016-06-09 01:45:00|[E4N, 48935]|E4N_48935_3|              3|      3|           91|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trafficData_E4N.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Feature Selection\n",
    "\n",
    "According to the general known knowledge on fundamental characteristics of traffic flow. There are three important features in relation to traffic flow.\n",
    "1. Flow - Number of moving vehicles\n",
    "2. Speed - Higher car speed indicated good traffic flow.\n",
    "3. Density - Concentration of the traffic. Too dense flow when approaching traffic jam and low density or free flow when no traffic.\n",
    "\n",
    "The three interrelated characteristics can be used to measure the traffic of a roadway. In this project, Density feature is added and its difference calculated to analyse time series data where a sequence of density difference in previous time steps is used together with the difference in current sequence to predict if there is traffic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Density Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_trafficData_E4N = df_trafficData_E4N.withColumn('Density', col('Flow_In')*60/col('Average_Speed'))\n",
    "w = Window.partitionBy('Ds_Reference', 'Detector_Number').orderBy('Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Time-Lag Column\n",
    "This is used to filter data that has time difference of 1-minute since the sensors record readings every minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+---------------+\n",
      "|          Timestamp|Ds_Reference|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|           Density|Time_Lag_Length|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+---------------+\n",
      "|2016-06-01 03:03:00|[E4N, 30710]|E4N_30710_2|              2|     18|          102|10.588235294117647|             60|\n",
      "|2016-06-01 03:04:00|[E4N, 30710]|E4N_30710_2|              2|     24|           96|              15.0|             60|\n",
      "|2016-06-01 03:05:00|[E4N, 30710]|E4N_30710_2|              2|     15|           97| 9.278350515463918|             60|\n",
      "|2016-06-01 03:06:00|[E4N, 30710]|E4N_30710_2|              2|      8|          105| 4.571428571428571|             60|\n",
      "|2016-06-01 03:07:00|[E4N, 30710]|E4N_30710_2|              2|     10|          100|               6.0|             60|\n",
      "|2016-06-01 03:08:00|[E4N, 30710]|E4N_30710_2|              2|      5|          110| 2.727272727272727|             60|\n",
      "|2016-06-01 03:09:00|[E4N, 30710]|E4N_30710_2|              2|      5|          101|2.9702970297029703|             60|\n",
      "|2016-06-01 03:10:00|[E4N, 30710]|E4N_30710_2|              2|      4|          105|2.2857142857142856|             60|\n",
      "|2016-06-01 03:11:00|[E4N, 30710]|E4N_30710_2|              2|      1|          119|0.5042016806722689|             60|\n",
      "|2016-06-01 03:12:00|[E4N, 30710]|E4N_30710_2|              2|      7|          101| 4.158415841584159|             60|\n",
      "|2016-06-01 03:13:00|[E4N, 30710]|E4N_30710_2|              2|      8|          101| 4.752475247524752|             60|\n",
      "|2016-06-01 03:14:00|[E4N, 30710]|E4N_30710_2|              2|      9|          103| 5.242718446601942|             60|\n",
      "|2016-06-01 03:15:00|[E4N, 30710]|E4N_30710_2|              2|      7|          102| 4.117647058823529|             60|\n",
      "|2016-06-01 03:16:00|[E4N, 30710]|E4N_30710_2|              2|      7|          108| 3.888888888888889|             60|\n",
      "|2016-06-01 03:17:00|[E4N, 30710]|E4N_30710_2|              2|     13|          105| 7.428571428571429|             60|\n",
      "|2016-06-01 03:18:00|[E4N, 30710]|E4N_30710_2|              2|      8|          103| 4.660194174757281|             60|\n",
      "|2016-06-01 03:19:00|[E4N, 30710]|E4N_30710_2|              2|     15|          105| 8.571428571428571|             60|\n",
      "|2016-06-01 03:20:00|[E4N, 30710]|E4N_30710_2|              2|     12|          101| 7.128712871287129|             60|\n",
      "|2016-06-01 03:21:00|[E4N, 30710]|E4N_30710_2|              2|     10|          110| 5.454545454545454|             60|\n",
      "|2016-06-01 03:22:00|[E4N, 30710]|E4N_30710_2|              2|     11|          111| 5.945945945945946|             60|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_difference = unix_timestamp('Timestamp', format='yyyy-MM-dd HH:mm:ss.SSS') - lag(unix_timestamp('Timestamp', format='yyyy-MM-dd HH:mm:ss.SSS')).over(w)                            \n",
    "df3_trafficData_E4N = df2_trafficData_E4N.withColumn('Time_Lag_Length', time_difference).filter(col('Time_Lag_Length') == 60)\n",
    "df3_trafficData_E4N.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the colums with features that will be used to train the model and save it to CSV file, to be read later for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------+---------------+-------+-------------+\n",
      "|          Timestamp|           Density|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|\n",
      "+-------------------+------------------+-----------+---------------+-------+-------------+\n",
      "|2016-06-01 03:03:00|10.588235294117647|E4N_30710_2|              2|     18|          102|\n",
      "|2016-06-01 03:04:00|              15.0|E4N_30710_2|              2|     24|           96|\n",
      "+-------------------+------------------+-----------+---------------+-------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df4_trafficData_E4N = df3_trafficData_E4N.drop(\"Ds_Reference\")\n",
    "df4_trafficData_E4N = df3_trafficData_E4N.select('Timestamp', 'Density', 'Ds_Ref', 'Detector_Number', 'Flow_In', 'Average_Speed').where('Status == 3 AND Ds_Reference.Road == \"E4N\"')\n",
    "df4_trafficData_E4N.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df4_trafficData_E4N.write.save('data/df4_trafficData_E4N.parquet', format='parquet')\n",
    "df = pd.read_parquet('data/df4_trafficData_E4N.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df4_trafficData_E4N.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data\n",
    "Required to get the right feature to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import L1L2\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a). Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15156387 entries, 0 to 15156386\n",
      "Data columns (total 7 columns):\n",
      "Unnamed: 0         int64\n",
      "Timestamp          datetime64[ns]\n",
      "Density            object\n",
      "Ds_Ref             object\n",
      "Detector_Number    int64\n",
      "Flow_In            int64\n",
      "Average_Speed      int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 809.4+ MB\n"
     ]
    }
   ],
   "source": [
    "def time_stamp_parser(time_stamp):\n",
    "    return datetime.strptime(time_stamp, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "traffic_series = read_csv('data/df4_trafficData_E4N.csv', header=0, parse_dates=[1],\n",
    "                  squeeze=True, decimal=',', date_parser=time_stamp_parser)\n",
    "\n",
    "traffic_series.head()\n",
    "traffic_series.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Density column from object type to float type, for easy mathematical operations such as getting density difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10.588235\n",
       "1    15.000000\n",
       "2     9.278351\n",
       "3     4.571429\n",
       "4     6.000000\n",
       "Name: Density, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_series1 = traffic_series.Density.astype(float)\n",
    "traffic_series1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b). Frame time series into as a supervised learning problem\n",
    "Convert data to be stationary whereby output from previous time step will be used as input of current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = concat(columns, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c). Calculate density difference\n",
    "Using the density feature, calculate the density difference to create the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDifference(dataset, interval=2):\n",
    "    density_diff = list() \n",
    "    for i in range(interval, len(dataset)):\n",
    "        density_value = dataset[i] - dataset[i - interval]\n",
    "        density_diff.append(density_value)\n",
    "    return Series(density_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Data\n",
    "Normalisation is performed to re-scale data into a distribution that states minimum and maximum values that need to be observed.\n",
    "Rescaling technique used here is MinMaxScaler from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to range from -1 to 1\n",
    "def scaleData(train, test):\n",
    "    # fit scaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    min_max_scaler = min_max_scaler.fit(train)\n",
    "    # transform train\n",
    "    # train[train_indices]\n",
    "    train = train.values.reshape(train.shape[0],train.shape[1])\n",
    "    \n",
    "    #train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_min_max_scaled_data = min_max_scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.values.reshape(test.shape[0],test.shape[1])\n",
    "    # test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_min_max_scaled_data = min_max_scaler.transform(test)\n",
    "    return min_max_scaler, train_min_max_scaled_data, test_min_max_scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Network\n",
    "Fit LSTM network into training data. The network takes in input data in 3D format, sample, time-steps and dimension(number of features), reshape method helps to shape the data.\n",
    "\n",
    "Layer 1: 8 neurons, sample size same as batch size, time-step X.shape[1] - based on the number of  detectors, X.shape[2] is the density feature.\n",
    "\n",
    "Layer 2: 4 neurons, the layer uses hard-sigmoid as an activation function as a way of mitigating vanishing/exploding gradient problem.\n",
    "\n",
    "Elasticnet regulizer to prevent over-fitting of the model to training data.\n",
    "\n",
    "The network uses Adam optimization algorithm and Mean Square-Error to measure error scores.\n",
    "\n",
    "##### Challenge: \n",
    "The network is experiencing Vanishing and Exploding gradient problem. Several techniques have been tested to solve the problem.\n",
    "1. Use of non-saturating activation function i.e hard-sigmoid.\n",
    "2. Gradient clipping - adam = Adam(lr=0.01, clipvalue=0.5), clips gradient between minimum of - 0.5 and maximum of 0.5\n",
    "3. Weight Initialization using Xavier Initialization i.e init='glorot_normal' in keras. A literature review revealed that batch Normalization is not good for LSTM Networks.\n",
    "\n",
    "Due to vanishing gradient problem, score errors to be used to plot a graph and see perfomance of the model and distribution of error where anormaly is expected to \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "def fit_lstm(train, batch_size, epochs, neurons,elasticnet_regularizer):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,recurrent_regularizer=elasticnet_regularizer))\n",
    "    # TODO X.shape[1]\n",
    "    adam = Adam(lr=0.01, clipvalue=0.5)\n",
    "    model.add(LSTM(8, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), init='glorot_normal', return_sequences=True,\n",
    "                   kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(LSTM(4, activation = 'hard_sigmoid', inner_activation = 'hard_sigmoid')) #return_sequences = True\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    for i in range(epochs):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Reverse \n",
    "Only important for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse values\n",
    "def reverse_density_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "# Reverse scaling for predicted values\n",
    "def reverse_scale(scaler, X, yhat):\n",
    "    create_row = [x for x in X] + [yhat]\n",
    "    array = numpy.array(create_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# run recurrent experiment\n",
    "def experiment(traffic_series, time_lag, recurrence, epochs, batch_size, neurons,elasticnet_regularizer):\n",
    "    # Convert time series to stationary data\n",
    "    \n",
    "    raw_density_values = traffic_series.values\n",
    "    differenced_density_values = getDifference(raw_density_values, 1)\n",
    "    \n",
    "    # Convert time series to supervised learning, prediction at previous timestep to be used current time step\n",
    "    supervised_data = timeseries_to_supervised(differenced_density_values, time_lag)\n",
    "   \n",
    "    # split data into train and test-sets\n",
    "    train, test = train_test_split(supervised_data, train_size=0.8)\n",
    "    \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scaleData(train, test)\n",
    "    \n",
    "    # run experiment\n",
    "    error_scores = list()\n",
    "    for r in range(recurrence):\n",
    "        # fit the model\n",
    "        train_trimmed = train_scaled[2:, :]\n",
    "        model = fit_lstm(train_trimmed, batch_size, epochs, neurons,elasticnet_regularizer)\n",
    "        \n",
    "        # make prediction on test dataset\n",
    "        test_reshaped = test_scaled[:,0:-1]\n",
    "        test_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)\n",
    "        output = model.predict(test_reshaped, batch_size=batch_size)\n",
    "        predictions = list()\n",
    "        # Reverse to original scale before calculating prediction error\n",
    "        for i in range(len(output)):\n",
    "            yhat = output[i,0]\n",
    "            X_input = test_scaled[i, 0:-1]\n",
    "            # Reverse scaling\n",
    "            yhat = reverse_scale(scaler, X_input, yhat)\n",
    "            # Reverse differencing density\n",
    "            yhat = reverse_density_difference(raw_density_values, yhat, len(test_scaled)+1-i)\n",
    "            # Save predictions\n",
    "            predictions.append(yhat)\n",
    "            # report performance\n",
    "        rmse = sqrt(mean_squared_error(raw_density_values[:], predictions))\n",
    "        print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "        error_scores.append(rmse)\n",
    "    return error_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the network and run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(8, batch_input_shape=(6, 1, 1), return_sequences=True, kernel_regularizer=<keras.reg..., activity_regularizer=<keras.reg..., kernel_initializer=\"glorot_normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(4, activation=\"hard_sigmoid\", recurrent_activation=\"hard_sigmoid\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (6, 1, 8)                 320       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (6, 4)                    208       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (6, 1)                    5         \n",
      "=================================================================\n",
      "Total params: 533\n",
      "Trainable params: 533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "  118050/12125106 [..............................] - ETA: 3:26:37 - loss: 1.5749e-04"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "#from pandas import datetime\n",
    "from datetime import datetime\n",
    "# configure the experiment\n",
    "def run():\n",
    "    # load dataset\n",
    "    #traffic_series = traffic_series1\n",
    "    #units=128\n",
    "    # configure the experiment\n",
    "    time_lag = 1\n",
    "    runExperiments = 30  # TODO: Run Experiment according to number of sensors  \n",
    "    epochs = 2 #1000\n",
    "    batch_size = 6\n",
    "    neurons = 50\n",
    "    elasticnet_regularizer = L1L2(l1=0.01, l2=0.01)\n",
    "    # run the experiment\n",
    "    results = DataFrame()\n",
    "    results['results'] = experiment(traffic_series1, time_lag, runExperiments, epochs, batch_size, neurons,elasticnet_regularizer)\n",
    "    # summarize results\n",
    "    print(results.describe())\n",
    "    \n",
    "run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above experiement run with different configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(4, activation=\"hard_sigmoid\", recurrent_activation=\"hard_sigmoid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (6, 1, 8)                 320       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (6, 4)                    208       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (6, 1)                    5         \n",
      "=================================================================\n",
      "Total params: 533\n",
      "Trainable params: 533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      " 4572354/12125106 [==========>...................] - ETA: 2:13:13 - loss: 6.2873e-06"
     ]
    }
   ],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "##### Output of density difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0          0\n",
      "13916465  -2.078049  -0.506567\n",
      "9753022    3.170732  -8.000000\n",
      "5887555    4.198758  -3.630847\n",
      "10175463  -5.316456   2.488341\n",
      "7027018    2.938053  -1.721519\n",
      "5193790   -2.666667   4.802260\n",
      "12840706  -3.166667   7.500000\n",
      "6885462   -1.522118  -0.083857\n",
      "5038497   -0.097371  -4.518014\n",
      "8144673    1.212121  -0.599251\n",
      "12712583  -4.567757   5.339034\n",
      "7397697   -0.666667   1.926740\n",
      "11175574   0.865385   0.157343\n",
      "7516057   -5.597561  -4.147059\n",
      "9317225    6.610797  -4.542569\n",
      "15101068  -4.550000   2.234848\n",
      "6278627   -0.025497   0.030888\n",
      "13660861 -15.353535  22.091503\n",
      "11600672   0.000000   0.825200\n",
      "4385440    5.521669  -6.103896\n",
      "2404482   -2.684932   3.052632\n",
      "5933896   -6.095662   5.806452\n",
      "2462454   -2.103387   5.426945\n",
      "15091987   9.647059 -20.804954\n",
      "2749573    0.260445  -1.344450\n",
      "121336    -0.385488   0.539503\n",
      "11087262  -6.251217  -5.226516\n",
      "5982344   -0.453782  -1.100213\n",
      "1082379    0.090909   6.309091\n",
      "3432260    0.838235   5.969178\n",
      "...             ...        ...\n",
      "4619816   29.743590 -28.531469\n",
      "141967     1.541502  -4.877622\n",
      "6847727   -0.199643   0.167189\n",
      "10227366   3.019884   1.654412\n",
      "5229294   -4.622642  -0.339806\n",
      "9486548   -0.714286   6.330724\n",
      "10403736  -1.665996   2.571429\n",
      "11192704 -11.265985   5.342632\n",
      "8903808   -4.615385   4.751131\n",
      "6382056   43.636364 -20.727273\n",
      "12932697  -2.373225   0.053476\n",
      "4466908   -0.450000   0.865385\n",
      "3664429    1.534091  -4.993598\n",
      "2188183    0.134656   0.705680\n",
      "497751    -4.272727   2.192308\n",
      "14891281   0.978109   1.145785\n",
      "6690534  -15.525151   5.081542\n",
      "13606831  -6.703297  22.857143\n",
      "10094388  -0.021060  -2.091038\n",
      "8088172    1.857678   2.162602\n",
      "722998    10.909091 -20.139860\n",
      "3790710    1.297186   0.157539\n",
      "7548898    3.525660   1.581081\n",
      "148994    -0.639948   3.671400\n",
      "14705397  -0.871622   0.713415\n",
      "15127924  -0.276680   4.668721\n",
      "10224624  -4.382775  -0.646465\n",
      "13624543  -0.783626   0.222222\n",
      "13082925  -5.027473  -2.453917\n",
      "5526255   -6.360306   5.818182\n",
      "\n",
      "[12125108 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-473a83e72e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-473a83e72e32>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_series1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunExperiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melasticnet_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-04b6a290d1b6>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(traffic_series, time_lag, recurrence, epochs, batch_size, neurons, elasticnet_regularizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# transform the scale of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# run experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c772cf3ac8e6>\u001b[0m in \u001b[0;36mscaleData\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# transform train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_min_max_scaled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# transform test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
