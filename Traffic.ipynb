{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf , col, lag, datediff, unix_timestamp,lit,coalesce,concat,split, explode\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_flow = StructType().add('Timestamp', TimestampType(), False) \\\n",
    "        .add('Ds_Reference', StringType(), False) \\\n",
    "        .add('Detector_Number', ShortType(), False) \\\n",
    "        .add('Traffic_Direction', ShortType(), False) \\\n",
    "        .add('Flow_In', ShortType(), False) \\\n",
    "        .add('Average_Speed', ShortType(), False) \\\n",
    "        .add('Sign_Aid_Det_Comms', ShortType(), False) \\\n",
    "        .add('Status', ShortType(), False) \\\n",
    "        .add('Legend_Group', ShortType(), False) \\\n",
    "        .add('Legend_Sign', ShortType(), False) \\\n",
    "        .add('Legend_SubSign', ShortType(), False) \\\n",
    "        .add('Protocol_Version', StringType(), False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Ds_Reference: string (nullable = true)\n",
      " |-- Detector_Number: short (nullable = true)\n",
      " |-- Traffic_Direction: short (nullable = true)\n",
      " |-- Flow_In: short (nullable = true)\n",
      " |-- Average_Speed: short (nullable = true)\n",
      " |-- Sign_Aid_Det_Comms: short (nullable = true)\n",
      " |-- Status: short (nullable = true)\n",
      " |-- Legend_Group: short (nullable = true)\n",
      " |-- Legend_Sign: short (nullable = true)\n",
      " |-- Legend_SubSign: short (nullable = true)\n",
      " |-- Protocol_Version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv('data/mcs_201606.csv', sep=';', schema=schema_flow, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS')\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 58.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85255773"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "|          Timestamp|Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "|2016-06-01 00:00:00| E182N 2,015|             49|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             50|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             51|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             52|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,325|             49|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "|Detector_Number|          Timestamp|Ds_Reference|Average_Speed|Flow_In|Status|\n",
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "|             49|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             52|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             49|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             49|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.select('Detector_Number','Timestamp','Ds_Reference','Average_Speed','Flow_In','Status').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-db7fefbaf454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df_raw.select('Timestamp').distinct.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_schema = StructType([\n",
    "  StructField('Road', StringType(), False),\n",
    "  StructField('Km_Ref', IntegerType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "@udf(split_schema)\n",
    "#@udf(StringType())\n",
    "def split_ds_ref(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        k, m = km.split(',')\n",
    "        meter = int(k)*1000 + int(m)\n",
    "        #var1[:] + meter \n",
    "        return r, meter\n",
    "    except:\n",
    "        return None\n",
    "var1 = ''\n",
    "#@udf(split_schema)\n",
    "@udf(StringType())\n",
    "def split_ds_ref2(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        return r\n",
    "    except:\n",
    "        return None \n",
    "@udf(StringType())\n",
    "def split_ds_ref3(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        k, m = km.split(',')\n",
    "        meter = int(k)*1000 + int(m)  \n",
    "        return meter\n",
    "    except:\n",
    "        return None     \n",
    "#def generate_sensor_ids1(*cols):\n",
    "#    return concat(*[coalesce(c, lit(\"*\")) for c in cols]) \n",
    "def generate_sensor_ids(s, d):\n",
    "    r, km = s.split(' ')\n",
    "    k, m = km.split(',')\n",
    "    meter = int(k)*1000 + int(m)\n",
    "    var1 = r, meter\n",
    "    return var\n",
    "\n",
    "funcConcatCols = udf(lambda x,y,z: x+'_'+y+'_'+z,StringType())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_to_int = udf(lambda x : x - 48, ShortType())\n",
    "df_cleanup1 = df_raw.withColumn('Detector_Number', ascii_to_int('Detector_Number'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup2 = df_cleanup1.withColumn('Ds_Ref_temp1',   split_ds_ref2('Ds_Reference')).withColumn('Ds_Ref_temp2',split_ds_ref3('Ds_Reference'))\n",
    "df_cleanup3 = df_cleanup2.withColumn('Ds_Ref', funcConcatCols(col('Ds_Ref_temp1'), col('Ds_Ref_temp2'),col('Detector_Number').cast(StringType())))\n",
    "df_cleanup3.show(2)\n",
    "\n",
    "df_cleanup4 = df_cleanup3.withColumn('Ds_Reference',split_ds_ref('Ds_Reference'))\n",
    "df_cleanup4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "speed_histogram = df_cleanup2.select('Status').rdd.flatMap(lambda x: x).histogram(5)\n",
    "speed_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(list(speed_histogram)[0], list(speed_histogram)[1])), \\\n",
    "             columns=['Status','Status Distibution']).set_index('Status').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fundamental Diagram of Traffic Flow\n",
    "\n",
    "![Traffic Flow Diagram](https://upload.wikimedia.org/wikipedia/commons/5/59/Fundamental_Diagram.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cleanup4.write.save('data/trafficData_E4N.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_trafficData_E4N = spark.read.parquet('data/trafficData_E4N.parquet').select('Timestamp', 'Ds_Reference', 'Ds_Ref', 'Detector_Number', 'Flow_In', 'Average_Speed').where('Status == 3 AND Ds_Reference.Road == \"E4N\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trafficData_E4N.createOrReplaceTempView(\"NormalTrafficFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trafficData_E4N.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Density Column\n",
    "df_trafficData_E4N = df_trafficData_E4N.withColumn('Density', col('Flow_In')*60/col('Average_Speed'))\n",
    "df_trafficData_E4N.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('Ds_Reference', 'Detector_Number').orderBy('Timestamp')\n",
    "densDiff = col('Density')- lag('Density', 1).over(w)\n",
    "timeFmt = 'yyyy-MM-dd HH:mm:ss.SSS'\n",
    "timeDiff = unix_timestamp('Timestamp', format=timeFmt) - lag(unix_timestamp('Timestamp', format=timeFmt)).over(w)\n",
    "                              \n",
    "df_diff = df_trafficData_E4N.withColumn('Density_Diff', densDiff).withColumn('timeDiff', timeDiff)\n",
    "df_diff.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sensor_ids = spark.sql('select distinct Ds_Ref from NormalTrafficFlow').rdd.map(lambda row: row).collect()\n",
    "sensor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sendor_ids = np.array(spark.sql('select distinct Ds_Ref from NormalTrafficFlow').rdd.map(lambda row: row[0]).collect())\n",
    "sendor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = spark.sql('select distinct _c1 from NormalTrafficFlow').rdd.map(lambda row: row._c1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "dim = 2\n",
    "lossHistory = LossHistory()\n",
    "# design network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "def train(data):\n",
    "    model.fit(data, data, epochs=20, batch_size=72, validation_data=(data, data), verbose=1, shuffle=False,callbacks=[lossHistory])\n",
    "\n",
    "def score(data):\n",
    "    yhat =  model.predict(data)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_trimmed_recording(df,file_id):\n",
    "    recording = np.array(df.orderBy(df['Timestamp']).where(df['Ds_Ref'] == file_id).select('Density','Density_Diff').rdd.map(lambda row: np.array([row.Density,row.Density_Diff])).collect())  \n",
    "    samples = len(recording)\n",
    "    print(samples)\n",
    "    trim = samples % 100\n",
    "    recording_trimmed = recording[:samples-trim]\n",
    "    print(recording_trimmed.shape)\n",
    "    recording_trimmed.shape = (samples//timesteps,timesteps,dim)\n",
    "    return recording_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_ids = spark.sql('select distinct _c1 from df_healthy').rdd.map(lambda row: row._c1).collect()\n",
    "start = time.time()\n",
    "for sensor_id in sendor_ids:\n",
    "    print(sensor_id)\n",
    "    recording_trimmed = create_trimmed_recording(df_diff,sensor_id)\n",
    "    print (\"Staring training on:\" + str(sensor_id))\n",
    "    train(recording_trimmed)\n",
    "    print (\"Finished training on\" + str(sensor_id) + \" after \" + str(time.time()-start) + \" seconds\")\n",
    "\n",
    "print (\"Finished job on after \" + str(time.time()-start) + \" seconds\")\n",
    "healthy_losses = lossHistory.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference,Ds_Ref, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM NormalTrafficFlow WHERE Status == 3 AND Ds_Reference.Road == \"E4N\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newDF = df_E4N.select(\"Ds_Reference\", \"Ds_Reference.*\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_ids = spark.sql('select Ds_Reference from NormalTrafficFlow').rdd.map(lambda x : x.Ds_Reference ).collect()\n",
    "#sensor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_E4N.count()\n",
    "df_E4N.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Density Column\n",
    "df_E4N_D = df_E4N.withColumn('Density', col('Flow_In')*60/col('Average_Speed'))\n",
    "df_E4N_D.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = Window.partitionBy('Ds_Reference', 'Detector_Number').orderBy('Timestamp')\n",
    "densDiff = col('Density')- lag('Density', 1).over(w)\n",
    "timeFmt = 'yyyy-MM-dd HH:mm:ss.SSS'\n",
    "timeDiff = unix_timestamp('Timestamp', format=timeFmt) - lag(unix_timestamp('Timestamp', format=timeFmt)).over(w)\n",
    "                              \n",
    "df_diff = df_E4N_D.withColumn('Density_Diff', densDiff).withColumn('timeDiff', timeDiff)\n",
    "df_diff.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#sensor_ids = np.array(df_diff.select('Ds_Ref').collect())\n",
    "#file_ids = spark.sql('select distinct _c1 from df_healthy').rdd.map(lambda row: row._c1).collect()\n",
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.flatMap(lambda row : row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_ids = spark.sql('select  distinct Ds_Ref from NormalTrafficFlow').rdd.map(lambda x : x._3).collect()\n",
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = np.array(df_diff.select('Ds_Ref').rdd.map(lambda row: np.array([row[0]])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#sensor_ids = np.array(df_diff.select('Ds_Ref').collect())\n",
    "#file_ids = spark.sql('select distinct _c1 from df_healthy').rdd.map(lambda row: row._c1).collect()\n",
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.flatMap(lambda row : row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.map(lambda row : row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trimmed_recording(df,file_id):\n",
    "    recording = np.array(df.df_diff(df['_c0']).where(df['_c1'] == file_id).select('_c2','_c3').rdd.map(lambda row: np.array([row._c2,row._c3])).collect())  \n",
    "    samples = len(recording)\n",
    "    #Ds_Ref\n",
    "    trim = samples % 100\n",
    "    recording_trimmed = recording[:samples-trim]\n",
    "    recording_trimmed.shape = (samples//timesteps,timesteps,dim)\n",
    "    return recording_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id from sensor_ids:\n",
    "    trimmed_recording = create_trimmed_record(df_diff, sensor_id)\n",
    "        recording_trimmed = create_trimmed_recording(df_healthy_read_parquet,file_id)\n",
    "    print (\"Staring training on:\" + str(file_id))\n",
    "    \n",
    "    train(recording_trimmed)\n",
    "    print (\"Finished training on\" + str(file_id) + \" after \" + str(time.time()-start) + \" seconds\")\n",
    "\n",
    "print (\"Finished job on after \" + str(time.time()-start) + \" seconds\")\n",
    "healthy_losses = lossHistory.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "dim = 2\n",
    "lossHistory = LossHistory()\n",
    "# design network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "def train(data):\n",
    "    model.fit(data, data, epochs=20, batch_size=72, validation_data=(data, data), verbose=1, shuffle=False,callbacks=[lossHistory])\n",
    "\n",
    "def score(data):\n",
    "    yhat =  model.predict(data)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM NormalTrafficFlow WHERE Status == 3 AND Ds_Reference.Road == \"E4N\"')\n",
    "\n",
    "sensor_ids = spark.sql('select distinct Sensor_ID from df_diff').rdd.map(lambda x : x.Sensor_ID ).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.withColumn('Ds_Reference', split_ds_ref('Ds_Reference'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%spark -o hist_df\n",
    "hist_df = spark.createDataFrame(list(zip(list(speed_histogram)[0], list(speed_histogram)[1])), \\\n",
    "             schema=['Status','Status distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.set_index('Status').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup2.createOrReplaceTempView(\"FlowData\")\n",
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM FlowData WHERE Status == 3 ')\n",
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM FlowData WHERE Status == 1 ')\n",
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM FlowData WHERE Status == 2 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
