{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf , col, lag, datediff, unix_timestamp,lit,coalesce,concat,split, explode\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_flow = StructType().add('Timestamp', TimestampType(), False) \\\n",
    "        .add('Ds_Reference', StringType(), False) \\\n",
    "        .add('Detector_Number', ShortType(), False) \\\n",
    "        .add('Traffic_Direction', ShortType(), False) \\\n",
    "        .add('Flow_In', ShortType(), False) \\\n",
    "        .add('Average_Speed', ShortType(), False) \\\n",
    "        .add('Sign_Aid_Det_Comms', ShortType(), False) \\\n",
    "        .add('Status', ShortType(), False) \\\n",
    "        .add('Legend_Group', ShortType(), False) \\\n",
    "        .add('Legend_Sign', ShortType(), False) \\\n",
    "        .add('Legend_SubSign', ShortType(), False) \\\n",
    "        .add('Protocol_Version', StringType(), False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Ds_Reference: string (nullable = true)\n",
      " |-- Detector_Number: short (nullable = true)\n",
      " |-- Traffic_Direction: short (nullable = true)\n",
      " |-- Flow_In: short (nullable = true)\n",
      " |-- Average_Speed: short (nullable = true)\n",
      " |-- Sign_Aid_Det_Comms: short (nullable = true)\n",
      " |-- Status: short (nullable = true)\n",
      " |-- Legend_Group: short (nullable = true)\n",
      " |-- Legend_Sign: short (nullable = true)\n",
      " |-- Legend_SubSign: short (nullable = true)\n",
      " |-- Protocol_Version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv('data/mcs_201606.csv', sep=';', schema=schema_flow, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS')\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "|          Timestamp|Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "|2016-06-01 00:00:00| E182N 2,015|             49|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             50|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             51|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,015|             52|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "|2016-06-01 00:00:00| E182N 2,325|             49|               78|      0|          252|                 0|     1|         255|          1|             1|               4|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "|Detector_Number|          Timestamp|Ds_Reference|Average_Speed|Flow_In|Status|\n",
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "|             49|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             52|2016-06-01 00:00:00| E182N 2,015|          252|      0|     1|\n",
      "|             49|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,325|          252|      0|     1|\n",
      "|             49|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "|             50|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "|             51|2016-06-01 00:00:00| E182N 2,690|          252|      0|     1|\n",
      "+---------------+-------------------+------------+-------------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.select('Detector_Number','Timestamp','Ds_Reference','Average_Speed','Flow_In','Status').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_schema = StructType([\n",
    "  StructField('Road', StringType(), False),\n",
    "  StructField('Km_Ref', IntegerType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "@udf(split_schema)\n",
    "#@udf(StringType())\n",
    "def split_ds_ref(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        k, m = km.split(',')\n",
    "        meter = int(k)*1000 + int(m)\n",
    "        #var1[:] + meter \n",
    "        return r, meter\n",
    "    except:\n",
    "        return None\n",
    "var1 = ''\n",
    "#@udf(split_schema)\n",
    "@udf(StringType())\n",
    "def split_ds_ref2(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        return r\n",
    "    except:\n",
    "        return None \n",
    "@udf(StringType())\n",
    "def split_ds_ref3(s):\n",
    "    try:\n",
    "        r, km = s.split(' ')\n",
    "        k, m = km.split(',')\n",
    "        meter = int(k)*1000 + int(m)  \n",
    "        return meter\n",
    "    except:\n",
    "        return None     \n",
    "#def generate_sensor_ids1(*cols):\n",
    "#    return concat(*[coalesce(c, lit(\"*\")) for c in cols]) \n",
    "def generate_sensor_ids(s, d):\n",
    "    r, km = s.split(' ')\n",
    "    k, m = km.split(',')\n",
    "    meter = int(k)*1000 + int(m)\n",
    "    var1 = r, meter\n",
    "    return var\n",
    "\n",
    "funcConcatCols = udf(lambda x,y,z: x+'_'+y+'_'+z,StringType())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_to_int = udf(lambda x : x - 48, ShortType())\n",
    "df_cleanup1 = df_raw.withColumn('Detector_Number', ascii_to_int('Detector_Number'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|          Timestamp|Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|Ds_Ref_temp1|Ds_Ref_temp2|      Ds_Ref|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|2016-06-01 00:00:00| E182N 2,015|              1|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_1|\n",
      "|2016-06-01 00:00:00| E182N 2,015|              2|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_2|\n",
      "+-------------------+------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+-------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|          Timestamp| Ds_Reference|Detector_Number|Traffic_Direction|Flow_In|Average_Speed|Sign_Aid_Det_Comms|Status|Legend_Group|Legend_Sign|Legend_SubSign|Protocol_Version|Ds_Ref_temp1|Ds_Ref_temp2|      Ds_Ref|\n",
      "+-------------------+-------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "|2016-06-01 00:00:00|[E182N, 2015]|              1|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_1|\n",
      "|2016-06-01 00:00:00|[E182N, 2015]|              2|               78|      0|          252|                 0|     1|         255|          1|             1|               4|       E182N|        2015|E182N_2015_2|\n",
      "+-------------------+-------------+---------------+-----------------+-------+-------------+------------------+------+------------+-----------+--------------+----------------+------------+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleanup2 = df_cleanup1.withColumn('Ds_Ref_temp1',   split_ds_ref2('Ds_Reference')).withColumn('Ds_Ref_temp2',split_ds_ref3('Ds_Reference'))\n",
    "df_cleanup3 = df_cleanup2.withColumn('Ds_Ref', funcConcatCols(col('Ds_Ref_temp1'), col('Ds_Ref_temp2'),col('Detector_Number').cast(StringType())))\n",
    "df_cleanup3.show(2)\n",
    "\n",
    "df_cleanup4 = df_cleanup3.withColumn('Ds_Reference',split_ds_ref('Ds_Reference'))\n",
    "df_cleanup4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#speed_histogram = df_cleanup2.select('Status').rdd.flatMap(lambda x: x).histogram(5)\n",
    "#speed_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(list(zip(list(speed_histogram)[0], list(speed_histogram)[1])), \\\n",
    "#             columns=['Status','Status Distibution']).set_index('Status').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup4.write.save('data/trafficData_E4N.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_trafficData_E4N = spark.read.parquet('data/trafficData_E4N.parquet').select('Timestamp', 'Ds_Reference', 'Ds_Ref', 'Detector_Number', 'Flow_In', 'Average_Speed').where('Status == 3 AND Ds_Reference.Road == \"E4N\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trafficData_E4N.createOrReplaceTempView(\"NormalTrafficFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+---------------+-------+-------------+\n",
      "|          Timestamp|Ds_Reference|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+\n",
      "|2016-06-09 01:45:00|[E4N, 47465]|E4N_47465_2|              2|      4|          101|\n",
      "|2016-06-09 01:45:00|[E4N, 47465]|E4N_47465_3|              3|      7|           84|\n",
      "|2016-06-09 01:45:00|[E4N, 47800]|E4N_47800_2|              2|      4|           98|\n",
      "|2016-06-09 01:45:00|[E4N, 47800]|E4N_47800_3|              3|      7|           91|\n",
      "|2016-06-09 01:45:00|[E4N, 48290]|E4N_48290_2|              2|      2|          104|\n",
      "|2016-06-09 01:45:00|[E4N, 48290]|E4N_48290_3|              3|      4|           97|\n",
      "|2016-06-09 01:45:00|[E4N, 48620]|E4N_48620_2|              2|      2|          119|\n",
      "|2016-06-09 01:45:00|[E4N, 48620]|E4N_48620_3|              3|      3|          105|\n",
      "|2016-06-09 01:45:00|[E4N, 48935]|E4N_48935_2|              2|      1|           96|\n",
      "|2016-06-09 01:45:00|[E4N, 48935]|E4N_48935_3|              3|      3|           91|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trafficData_E4N.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+---------------+\n",
      "|          Timestamp|Ds_Reference|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|           Density|Time_Lag_Length|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+---------------+\n",
      "|2016-06-01 03:03:00|[E4N, 30710]|E4N_30710_2|              2|     18|          102|10.588235294117647|             60|\n",
      "|2016-06-01 03:04:00|[E4N, 30710]|E4N_30710_2|              2|     24|           96|              15.0|             60|\n",
      "|2016-06-01 03:05:00|[E4N, 30710]|E4N_30710_2|              2|     15|           97| 9.278350515463918|             60|\n",
      "|2016-06-01 03:06:00|[E4N, 30710]|E4N_30710_2|              2|      8|          105| 4.571428571428571|             60|\n",
      "|2016-06-01 03:07:00|[E4N, 30710]|E4N_30710_2|              2|     10|          100|               6.0|             60|\n",
      "|2016-06-01 03:08:00|[E4N, 30710]|E4N_30710_2|              2|      5|          110| 2.727272727272727|             60|\n",
      "|2016-06-01 03:09:00|[E4N, 30710]|E4N_30710_2|              2|      5|          101|2.9702970297029703|             60|\n",
      "|2016-06-01 03:10:00|[E4N, 30710]|E4N_30710_2|              2|      4|          105|2.2857142857142856|             60|\n",
      "|2016-06-01 03:11:00|[E4N, 30710]|E4N_30710_2|              2|      1|          119|0.5042016806722689|             60|\n",
      "|2016-06-01 03:12:00|[E4N, 30710]|E4N_30710_2|              2|      7|          101| 4.158415841584159|             60|\n",
      "|2016-06-01 03:13:00|[E4N, 30710]|E4N_30710_2|              2|      8|          101| 4.752475247524752|             60|\n",
      "|2016-06-01 03:14:00|[E4N, 30710]|E4N_30710_2|              2|      9|          103| 5.242718446601942|             60|\n",
      "|2016-06-01 03:15:00|[E4N, 30710]|E4N_30710_2|              2|      7|          102| 4.117647058823529|             60|\n",
      "|2016-06-01 03:16:00|[E4N, 30710]|E4N_30710_2|              2|      7|          108| 3.888888888888889|             60|\n",
      "|2016-06-01 03:17:00|[E4N, 30710]|E4N_30710_2|              2|     13|          105| 7.428571428571429|             60|\n",
      "|2016-06-01 03:18:00|[E4N, 30710]|E4N_30710_2|              2|      8|          103| 4.660194174757281|             60|\n",
      "|2016-06-01 03:19:00|[E4N, 30710]|E4N_30710_2|              2|     15|          105| 8.571428571428571|             60|\n",
      "|2016-06-01 03:20:00|[E4N, 30710]|E4N_30710_2|              2|     12|          101| 7.128712871287129|             60|\n",
      "|2016-06-01 03:21:00|[E4N, 30710]|E4N_30710_2|              2|     10|          110| 5.454545454545454|             60|\n",
      "|2016-06-01 03:22:00|[E4N, 30710]|E4N_30710_2|              2|     11|          111| 5.945945945945946|             60|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Density Column\n",
    "df2_trafficData_E4N = df_trafficData_E4N.withColumn('Density', col('Flow_In')*60/col('Average_Speed'))\n",
    "\n",
    "w = Window.partitionBy('Ds_Reference', 'Detector_Number').orderBy('Timestamp')\n",
    "time_diff = unix_timestamp('Timestamp', format='yyyy-MM-dd HH:mm:ss.SSS') - lag(unix_timestamp('Timestamp', format='yyyy-MM-dd HH:mm:ss.SSS')).over(w)                            \n",
    "df3_trafficData_E4N = df2_trafficData_E4N.withColumn('Time_Lag_Length', time_diff).filter(col('Time_Lag_Length') == 60)\n",
    "df3_trafficData_E4N.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+----------------+\n",
      "|          Timestamp|Ds_Reference|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|           Density|Time_Lag_Length |\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+----------------+\n",
      "|2016-06-01 00:48:00|[E4N, 30710]|E4N_30710_2|              2|      1|           28| 2.142857142857143|            null|\n",
      "|2016-06-01 02:27:00|[E4N, 30710]|E4N_30710_2|              2|      1|           36|1.6666666666666667|            5940|\n",
      "|2016-06-01 02:33:00|[E4N, 30710]|E4N_30710_2|              2|      1|           99|0.6060606060606061|             360|\n",
      "|2016-06-01 03:02:00|[E4N, 30710]|E4N_30710_2|              2|     11|          113|  5.84070796460177|            1740|\n",
      "|2016-06-01 03:03:00|[E4N, 30710]|E4N_30710_2|              2|     18|          102|10.588235294117647|              60|\n",
      "|2016-06-01 03:04:00|[E4N, 30710]|E4N_30710_2|              2|     24|           96|              15.0|              60|\n",
      "|2016-06-01 03:05:00|[E4N, 30710]|E4N_30710_2|              2|     15|           97| 9.278350515463918|              60|\n",
      "|2016-06-01 03:06:00|[E4N, 30710]|E4N_30710_2|              2|      8|          105| 4.571428571428571|              60|\n",
      "|2016-06-01 03:07:00|[E4N, 30710]|E4N_30710_2|              2|     10|          100|               6.0|              60|\n",
      "|2016-06-01 03:08:00|[E4N, 30710]|E4N_30710_2|              2|      5|          110| 2.727272727272727|              60|\n",
      "|2016-06-01 03:09:00|[E4N, 30710]|E4N_30710_2|              2|      5|          101|2.9702970297029703|              60|\n",
      "|2016-06-01 03:10:00|[E4N, 30710]|E4N_30710_2|              2|      4|          105|2.2857142857142856|              60|\n",
      "|2016-06-01 03:11:00|[E4N, 30710]|E4N_30710_2|              2|      1|          119|0.5042016806722689|              60|\n",
      "|2016-06-01 03:12:00|[E4N, 30710]|E4N_30710_2|              2|      7|          101| 4.158415841584159|              60|\n",
      "|2016-06-01 03:13:00|[E4N, 30710]|E4N_30710_2|              2|      8|          101| 4.752475247524752|              60|\n",
      "|2016-06-01 03:14:00|[E4N, 30710]|E4N_30710_2|              2|      9|          103| 5.242718446601942|              60|\n",
      "|2016-06-01 03:15:00|[E4N, 30710]|E4N_30710_2|              2|      7|          102| 4.117647058823529|              60|\n",
      "|2016-06-01 03:16:00|[E4N, 30710]|E4N_30710_2|              2|      7|          108| 3.888888888888889|              60|\n",
      "|2016-06-01 03:17:00|[E4N, 30710]|E4N_30710_2|              2|     13|          105| 7.428571428571429|              60|\n",
      "|2016-06-01 03:18:00|[E4N, 30710]|E4N_30710_2|              2|      8|          103| 4.660194174757281|              60|\n",
      "+-------------------+------------+-----------+---------------+-------+-------------+------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_diff.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------+---------------+-------+-------------+\n",
      "|          Timestamp|           Density|     Ds_Ref|Detector_Number|Flow_In|Average_Speed|\n",
      "+-------------------+------------------+-----------+---------------+-------+-------------+\n",
      "|2016-06-01 03:03:00|10.588235294117647|E4N_30710_2|              2|     18|          102|\n",
      "|2016-06-01 03:04:00|              15.0|E4N_30710_2|              2|     24|           96|\n",
      "+-------------------+------------------+-----------+---------------+-------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df4_trafficData_E4N = df3_trafficData_E4N.drop(\"Ds_Reference\")\n",
    "df4_trafficData_E4N = df3_trafficData_E4N.select('Timestamp', 'Density', 'Ds_Ref', 'Detector_Number', 'Flow_In', 'Average_Speed').where('Status == 3 AND Ds_Reference.Road == \"E4N\"')\n",
    "df4_trafficData_E4N.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4_trafficData_E4N.coalesce(1).write.format(\"com.databricks.spark.csv\").save(\"data/df4_trafficData_E4N.csv\") memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df4_trafficData_E4N.write.save('data/df4_trafficData_E4N.parquet', format='parquet')\n",
    "df = pd.read_parquet('data/df4_trafficData_E4N.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df4_trafficData_E4N.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import L1L2\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = concat(columns, axis=1)\n",
    "    return df\n",
    " \n",
    "# create a differenced series\n",
    "def getDifference(dataset, interval=2):\n",
    "    density_diff = list() \n",
    "    for i in range(interval, len(dataset)):\n",
    "        density_value = dataset[i] - dataset[i - interval]\n",
    "        density_diff.append(density_value)\n",
    "    return Series(density_diff)\n",
    " \n",
    "# Reverse values\n",
    "def reverse_density_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    " \n",
    "# scale data to [-1, 1]\n",
    "def scaleData(train, test):\n",
    "    # fit scaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    min_max_scaler = min_max_scaler.fit(train)\n",
    "    # transform train\n",
    "    # train[train_indices]\n",
    "    train = train.values.reshape(train.shape[0],train.shape[1])\n",
    "    \n",
    "    #train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_min_max_scaled_data = min_max_scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.values.reshape(test.shape[0],test.shape[1])\n",
    "    # test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_min_max_scaled_data = min_max_scaler.transform(test)\n",
    "    return min_max_scaler, train_min_max_scaled_data, test_min_max_scaled_data\n",
    " \n",
    "# Reverse scaling for predicted values\n",
    "def reverse_scale(scaler, X, yhat):\n",
    "    create_row = [x for x in X] + [yhat]\n",
    "    array = numpy.array(create_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]\n",
    "from keras.optimizers import Adam\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, epochs, neurons,elasticnet_regularizer):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,recurrent_regularizer=elasticnet_regularizer))\n",
    "    adam = Adam(lr=0.01, clipvalue=0.5)\n",
    "    model.add(LSTM(8, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), return_sequences=True,\n",
    "                   kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(LSTM(4, activation = 'hard_sigmoid', inner_activation = 'hard_sigmoid')) #return_sequences = True\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    for i in range(epochs):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# run recurrent experiment\n",
    "def experiment(traffic_series, time_lag, recurrence, epochs, batch_size, neurons,elasticnet_regularizer):\n",
    "    # Convert time series to stationary data\n",
    "    \n",
    "    raw_density_values = traffic_series.values\n",
    "    differenced_density_values = getDifference(raw_density_values, 1)\n",
    "    \n",
    "    # Convert time series to supervised learning, prediction at previous timestep to be used current time step\n",
    "    supervised_data = timeseries_to_supervised(differenced_density_values, time_lag)\n",
    "   \n",
    "    # split data into train and test-sets\n",
    "    train, test = train_test_split(supervised_data, train_size=0.8)\n",
    "    \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scaleData(train, test)\n",
    "    \n",
    "    # run experiment\n",
    "    error_scores = list()\n",
    "    for r in range(recurrence):\n",
    "        # fit the model\n",
    "        train_trimmed = train_scaled[2:, :]\n",
    "        model = fit_lstm(train_trimmed, batch_size, epochs, neurons,elasticnet_regularizer)\n",
    "        \n",
    "        # make prediction on test dataset\n",
    "        test_reshaped = test_scaled[:,0:-1]\n",
    "        test_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)\n",
    "        output = model.predict(test_reshaped, batch_size=batch_size)\n",
    "        predictions = list()\n",
    "        # Reverse to original scale before calculating prediction error\n",
    "        for i in range(len(output)):\n",
    "            yhat = output[i,0]\n",
    "            X_input = test_scaled[i, 0:-1]\n",
    "            # Reverse scaling\n",
    "            yhat = reverse_scale(scaler, X_input, yhat)\n",
    "            # Reverse differencing density\n",
    "            yhat = reverse_density_difference(raw_density_values, yhat, len(test_scaled)+1-i)\n",
    "            # Save predictions\n",
    "            predictions.append(yhat)\n",
    "            # report performance\n",
    "        rmse = sqrt(mean_squared_error(raw_density_values[:], predictions))\n",
    "        print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "        error_scores.append(rmse)\n",
    "    return error_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15156387 entries, 0 to 15156386\n",
      "Data columns (total 7 columns):\n",
      "Unnamed: 0         int64\n",
      "Timestamp          datetime64[ns]\n",
      "Density            object\n",
      "Ds_Ref             object\n",
      "Detector_Number    int64\n",
      "Flow_In            int64\n",
      "Average_Speed      int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 809.4+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def time_stamp_parser(time_stamp):\n",
    "    return datetime.strptime(time_stamp, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "traffic_series = read_csv('data/df4_trafficData_E4N.csv', header=0, parse_dates=[1],\n",
    "                  squeeze=True, decimal=',', date_parser=time_stamp_parser)\n",
    "\n",
    "traffic_series.head()\n",
    "traffic_series.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10.588235\n",
       "1    15.000000\n",
       "2     9.278351\n",
       "3     4.571429\n",
       "4     6.000000\n",
       "Name: Density, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_series1 = traffic_series.Density.astype(float)\n",
    "traffic_series1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(4, activation=\"hard_sigmoid\", recurrent_activation=\"hard_sigmoid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (6, 1, 8)                 320       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (6, 4)                    208       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (6, 1)                    5         \n",
      "=================================================================\n",
      "Total params: 533\n",
      "Trainable params: 533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      " 4572354/12125106 [==========>...................] - ETA: 2:13:13 - loss: 6.2873e-06"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "#from pandas import datetime\n",
    "from datetime import datetime\n",
    "# configure the experiment\n",
    "def run():\n",
    "    # load dataset\n",
    "    #traffic_series = traffic_series1\n",
    "    #units=128\n",
    "    # configure the experiment\n",
    "    time_lag = 1\n",
    "    runExperiments = 30  # TODO: Run Experiment according to number of sensors  \n",
    "    epochs = 2 #1000\n",
    "    batch_size = 6\n",
    "    neurons = 50\n",
    "    elasticnet_regularizer = L1L2(l1=0.01, l2=0.01)\n",
    "    # run the experiment\n",
    "    results = DataFrame()\n",
    "    results['results'] = experiment(traffic_series1, time_lag, runExperiments, epochs, batch_size, neurons,elasticnet_regularizer)\n",
    "    # summarize results\n",
    "    print(results.describe())\n",
    "    \n",
    "run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10.588235\n",
       "1    15.000000\n",
       "2     9.278351\n",
       "3     4.571429\n",
       "4     6.000000\n",
       "Name: Density, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_series1 = traffic_series.Density.astype(float)\n",
    "traffic_series1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (32077, 50)               10400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32077, 1)                51        \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 59s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 59s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 59s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 58s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 59s 5us/step - loss: nan\n",
      "Epoch 1/1\n",
      "12125106/12125106 [==============================] - 59s 5us/step - loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 3031278 samples. Batch size: 32077.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-30fc2cd07411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-30fc2cd07411>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_series1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunExperiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melasticnet_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-02c93151874f>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(traffic_series, time_lag, recurrence, epochs, batch_size, neurons, elasticnet_regularizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtest_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtest_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Reverse to original scale before calculating prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1823\u001b[0m                                  \u001b[0;34m'divided by the batch size. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' samples. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1825\u001b[0;31m                                  'Batch size: ' + str(batch_size) + '.')\n\u001b[0m\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_predict_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 3031278 samples. Batch size: 32077."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "#from pandas import datetime\n",
    "from datetime import datetime\n",
    "# configure the experiment\n",
    "def run():\n",
    "    # load dataset\n",
    "    #traffic_series = traffic_series1\n",
    "    #units=128\n",
    "    # configure the experiment\n",
    "    time_lag = 1\n",
    "    runExperiments = 30  # TODO: Run Experiment according to number of sensors  \n",
    "    epochs = 10 #1000\n",
    "    batch_size = 32077\n",
    "    neurons = 50\n",
    "    elasticnet_regularizer = L1L2(l1=0.01, l2=0.01)\n",
    "    # run the experiment\n",
    "    results = DataFrame()\n",
    "    results['results'] = experiment(traffic_series1, time_lag, runExperiments, epochs, batch_size, neurons,elasticnet_regularizer)\n",
    "    # summarize results\n",
    "    print(results.describe())\n",
    "    \n",
    "run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15156387 entries, 0 to 15156386\n",
      "Data columns (total 7 columns):\n",
      "Unnamed: 0         int64\n",
      "Timestamp          datetime64[ns]\n",
      "Density            object\n",
      "Ds_Ref             object\n",
      "Detector_Number    int64\n",
      "Flow_In            int64\n",
      "Average_Speed      int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 809.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#def time_stamp_parser(time_stamp):\n",
    "#    return datetime.strptime(time_stamp, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#traffic_series = read_csv('data/df4_trafficData_E4N.csv', header=0, parse_dates=[1],\n",
    "#                  squeeze=True, decimal=',', date_parser=time_stamp_parser)\n",
    "\n",
    "#traffic_series.head()\n",
    "#traffic_series.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0          0\n",
      "13916465  -2.078049  -0.506567\n",
      "9753022    3.170732  -8.000000\n",
      "5887555    4.198758  -3.630847\n",
      "10175463  -5.316456   2.488341\n",
      "7027018    2.938053  -1.721519\n",
      "5193790   -2.666667   4.802260\n",
      "12840706  -3.166667   7.500000\n",
      "6885462   -1.522118  -0.083857\n",
      "5038497   -0.097371  -4.518014\n",
      "8144673    1.212121  -0.599251\n",
      "12712583  -4.567757   5.339034\n",
      "7397697   -0.666667   1.926740\n",
      "11175574   0.865385   0.157343\n",
      "7516057   -5.597561  -4.147059\n",
      "9317225    6.610797  -4.542569\n",
      "15101068  -4.550000   2.234848\n",
      "6278627   -0.025497   0.030888\n",
      "13660861 -15.353535  22.091503\n",
      "11600672   0.000000   0.825200\n",
      "4385440    5.521669  -6.103896\n",
      "2404482   -2.684932   3.052632\n",
      "5933896   -6.095662   5.806452\n",
      "2462454   -2.103387   5.426945\n",
      "15091987   9.647059 -20.804954\n",
      "2749573    0.260445  -1.344450\n",
      "121336    -0.385488   0.539503\n",
      "11087262  -6.251217  -5.226516\n",
      "5982344   -0.453782  -1.100213\n",
      "1082379    0.090909   6.309091\n",
      "3432260    0.838235   5.969178\n",
      "...             ...        ...\n",
      "4619816   29.743590 -28.531469\n",
      "141967     1.541502  -4.877622\n",
      "6847727   -0.199643   0.167189\n",
      "10227366   3.019884   1.654412\n",
      "5229294   -4.622642  -0.339806\n",
      "9486548   -0.714286   6.330724\n",
      "10403736  -1.665996   2.571429\n",
      "11192704 -11.265985   5.342632\n",
      "8903808   -4.615385   4.751131\n",
      "6382056   43.636364 -20.727273\n",
      "12932697  -2.373225   0.053476\n",
      "4466908   -0.450000   0.865385\n",
      "3664429    1.534091  -4.993598\n",
      "2188183    0.134656   0.705680\n",
      "497751    -4.272727   2.192308\n",
      "14891281   0.978109   1.145785\n",
      "6690534  -15.525151   5.081542\n",
      "13606831  -6.703297  22.857143\n",
      "10094388  -0.021060  -2.091038\n",
      "8088172    1.857678   2.162602\n",
      "722998    10.909091 -20.139860\n",
      "3790710    1.297186   0.157539\n",
      "7548898    3.525660   1.581081\n",
      "148994    -0.639948   3.671400\n",
      "14705397  -0.871622   0.713415\n",
      "15127924  -0.276680   4.668721\n",
      "10224624  -4.382775  -0.646465\n",
      "13624543  -0.783626   0.222222\n",
      "13082925  -5.027473  -2.453917\n",
      "5526255   -6.360306   5.818182\n",
      "\n",
      "[12125108 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-473a83e72e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-473a83e72e32>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_series1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunExperiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melasticnet_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-04b6a290d1b6>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(traffic_series, time_lag, recurrence, epochs, batch_size, neurons, elasticnet_regularizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# transform the scale of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# run experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c772cf3ac8e6>\u001b[0m in \u001b[0;36mscaleData\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# transform train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_min_max_scaled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# transform test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densDiff = col('Density')- lag('Density', 1).over(w)\n",
    " \n",
    "time_diff = unix_timestamp('Timestamp', format='yyyy-MM-dd HH:mm:ss.SSS') - lag(unix_timestamp('Timestamp', format=timeFmt)).over(w)\n",
    "                              \n",
    "df_diff = df_trafficData_E4N.withColumn('Density_Diff', densDiff).withColumn('timeDiff', timeDiff)\n",
    "df_diff.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sensor_ids = spark.sql('select distinct Ds_Ref from NormalTrafficFlow').rdd.map(lambda row: row).collect()\n",
    "sensor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sendor_ids = np.array(spark.sql('select distinct Ds_Ref from NormalTrafficFlow').rdd.map(lambda row: row[0]).collect())\n",
    "sendor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = spark.sql('select distinct _c1 from NormalTrafficFlow').rdd.map(lambda row: row._c1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "dim = 2\n",
    "lossHistory = LossHistory()\n",
    "# design network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "def train(data):\n",
    "    model.fit(data, data, epochs=20, batch_size=72, validation_data=(data, data), verbose=1, shuffle=False,callbacks=[lossHistory])\n",
    "\n",
    "def score(data):\n",
    "    yhat =  model.predict(data)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_trafficData_E4N = spark.read.parquet('data/df4_trafficData_E4N.parquet').select('Timestamp', 'Density')\n",
    "df5_trafficData_E4N.createOrReplaceTempView(\"TransformedNormalTrafficFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_trimmed_recording():\n",
    "    density_series = np.array(df5_trafficData_E4N.select('Timestamp', 'Density').rdd.map(lambda row: np.array([row.Timestamp,row.Density])).collect())\n",
    "    print(density_series)\n",
    "    samples = len(density_series)\n",
    "    trimmed_samples_of_100s = samples % 100\n",
    "    trimmed_density_series = density_series[:samples-trimmed_samples_of_100s]\n",
    "    recording_trimmed.shape = (samples//timesteps,timesteps,dim)\n",
    "    return recording_trimmed\n",
    "print(create_trimmed_recording())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_trimmed_recording(df,file_id):\n",
    "    recording = np.array(df.orderBy(df['Timestamp']).where(df['Ds_Ref'] == file_id).select('Density','Density_Diff').rdd.map(lambda row: np.array([row.Density,row.Density_Diff])).collect())  \n",
    "    \n",
    "    samples = len(recording)\n",
    "    print(samples)\n",
    "    trim = samples % 100\n",
    "    recording_trimmed = recording[:samples-trim]\n",
    "    print(recording_trimmed.shape)\n",
    "    recording_trimmed.shape = (samples//timesteps,timesteps,dim)\n",
    "    return recording_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_ids = spark.sql('select distinct _c1 from df_healthy').rdd.map(lambda row: row._c1).collect()\n",
    "start = time.time()\n",
    "for sensor_id in sendor_ids:\n",
    "    print(sensor_id)\n",
    "    recording_trimmed = create_trimmed_recording(df_diff,sensor_id)\n",
    "    print (\"Staring training on:\" + str(sensor_id))\n",
    "    train(recording_trimmed)\n",
    "    print (\"Finished training on\" + str(sensor_id) + \" after \" + str(time.time()-start) + \" seconds\")\n",
    "\n",
    "print (\"Finished job on after \" + str(time.time()-start) + \" seconds\")\n",
    "healthy_losses = lossHistory.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference,Ds_Ref, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM NormalTrafficFlow WHERE Status == 3 AND Ds_Reference.Road == \"E4N\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newDF = df_E4N.select(\"Ds_Reference\", \"Ds_Reference.*\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_ids = spark.sql('select Ds_Reference from NormalTrafficFlow').rdd.map(lambda x : x.Ds_Reference ).collect()\n",
    "#sensor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_E4N.count()\n",
    "df_E4N.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Density Column\n",
    "df_E4N_D = df_E4N.withColumn('Density', col('Flow_In')*60/col('Average_Speed'))\n",
    "df_E4N_D.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = Window.partitionBy('Ds_Reference', 'Detector_Number').orderBy('Timestamp')\n",
    "densDiff = col('Density')- lag('Density', 1).over(w)\n",
    "timeFmt = 'yyyy-MM-dd HH:mm:ss.SSS'\n",
    "timeDiff = unix_timestamp('Timestamp', format=timeFmt) - lag(unix_timestamp('Timestamp', format=timeFmt)).over(w)\n",
    "                              \n",
    "df_diff = df_E4N_D.withColumn('Density_Diff', densDiff).withColumn('timeDiff', timeDiff)\n",
    "df_diff.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#sensor_ids = np.array(df_diff.select('Ds_Ref').collect())\n",
    "#file_ids = spark.sql('select distinct _c1 from df_healthy').rdd.map(lambda row: row._c1).collect()\n",
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.flatMap(lambda row : row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_ids = spark.sql('select  distinct Ds_Ref from NormalTrafficFlow').rdd.map(lambda x : x._3).collect()\n",
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = np.array(df_diff.select('Ds_Ref').rdd.map(lambda row: np.array([row[0]])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#sensor_ids = np.array(df_diff.select('Ds_Ref').collect())\n",
    "#file_ids = spark.sql('select distinct _c1 from df_healthy').rdd.map(lambda row: row._c1).collect()\n",
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.flatMap(lambda row : row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ids = df_diff.select(\"Ds_Ref\").rdd.map(lambda row : row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trimmed_recording(df,file_id):\n",
    "    recording = np.array(df.df_diff(df['_c0']).where(df['_c1'] == file_id).select('_c2','_c3').rdd.map(lambda row: np.array([row._c2,row._c3])).collect())  \n",
    "    samples = len(recording)\n",
    "    #Ds_Ref\n",
    "    trim = samples % 100\n",
    "    recording_trimmed = recording[:samples-trim]\n",
    "    recording_trimmed.shape = (samples//timesteps,timesteps,dim)\n",
    "    return recording_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id from sensor_ids:\n",
    "    trimmed_recording = create_trimmed_record(df_diff, sensor_id)\n",
    "        recording_trimmed = create_trimmed_recording(df_healthy_read_parquet,file_id)\n",
    "    print (\"Staring training on:\" + str(file_id))\n",
    "    \n",
    "    train(recording_trimmed)\n",
    "    print (\"Finished training on\" + str(file_id) + \" after \" + str(time.time()-start) + \" seconds\")\n",
    "\n",
    "print (\"Finished job on after \" + str(time.time()-start) + \" seconds\")\n",
    "healthy_losses = lossHistory.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "dim = 2\n",
    "lossHistory = LossHistory()\n",
    "# design network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "def train(data):\n",
    "    model.fit(data, data, epochs=20, batch_size=72, validation_data=(data, data), verbose=1, shuffle=False,callbacks=[lossHistory])\n",
    "\n",
    "def score(data):\n",
    "    yhat =  model.predict(data)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM NormalTrafficFlow WHERE Status == 3 AND Ds_Reference.Road == \"E4N\"')\n",
    "\n",
    "sensor_ids = spark.sql('select distinct Sensor_ID from df_diff').rdd.map(lambda x : x.Sensor_ID ).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.withColumn('Ds_Reference', split_ds_ref('Ds_Reference'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%spark -o hist_df\n",
    "hist_df = spark.createDataFrame(list(zip(list(speed_histogram)[0], list(speed_histogram)[1])), \\\n",
    "             schema=['Status','Status distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.set_index('Status').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup2.createOrReplaceTempView(\"FlowData\")\n",
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM FlowData WHERE Status == 3 ')\n",
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM FlowData WHERE Status == 1 ')\n",
    "df_E4N = spark.sql('SELECT Timestamp, Ds_Reference, Detector_Number, Flow_In, Average_Speed ' \n",
    "                  'FROM FlowData WHERE Status == 2 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
