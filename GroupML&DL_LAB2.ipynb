{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow and Deep Learning\n",
    "\n",
    "In this lab assignment, first you will learn how to build and train a neural network that recognises handwritten digits, and then you will build LeNet-5 CNN architecture, which is widely used for handwritten digit recognition. At the end of this lab assignment, you will make AlexNet CNN architecture, which won the 2012 ImageNet ILSVRC challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Dataset\n",
    "In the first part of the assignment, we use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. There are 70,000 images, and each image has 784 features. This is because each image is 28×28=784 pixels, and each feature simply represents one pixel's intensity, from 0 (white) to 255 (black). The following figure shows a few images from the MNIST dataset to give you a feel for the complexity of the classification task.\n",
    "\n",
    "<img src=\"figs/1-mnist.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "To begin the assignment, first, use `mnist_data.read_data_sets` and download images and labels. It return two lists, called `mnist.test` with 10K images+labels, and `mnist.train` with 60K images+labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-09f9bc876940>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "\n",
    "mnist = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. A One-Layer Neural Network\n",
    "<img src=\"figs/2-comic1.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Let's start by building a one-layer neural network. Handwritten digits in the MNIST dataset are 28x28 pixel greyscale images. The simplest approach for classifying them is to use the 28x28=784 pixels as inputs for a **one-layer neural network**. Each neuron in the network does a weighted sum of all of its inputs, adds a bias and then feeds the result through some non-linear activation function. Here we design a one-layer neural network with 10 output neurons since we want to classify digits into 10 classes (0 to 9).\n",
    "<img src=\"figs/3-one_layer.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "For a classification problem, an *activation function* that works well is **softmax**. Applying softmax on a vector is done by taking the exponential of each element and then normalising the vector.\n",
    "<img src=\"figs/4-softmax.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "We can summarise the behaviour of this single layer of neurons into a simple formula using a *matrix multiply*. If we give input data into the network in *mini-batch* of 100 images, it produces 100 predictions as the output. We define the **weights matrix $W$** with 10 columns, in which each column indicates the weight of a one class (a single digit), from 0 to 9. Using the first column of $W$, we can compute the weighted sum of all the pixels of the first image. This sum corresponds to the first neuron that points to the number 0. Using the second column of $W$, we do the same for the second neuron (number 1) and so on until the 10th neuron. We can then repeat the operation for the remaining 99 images in the mini-batch. If we call $X$ the matrix containing our 100 images (each row corresponds to one digit), all the weighted sums for our 10 neurons, computed on 100 images are simply $X.W$. Each neuron must now add its bias. Since we have 10 neurons, we have 10 bias constants. We finally apply the **softmax activation function** and obtain the formula describing a one-layer neural network, applied to 100 images.\n",
    "<img src=\"figs/5-xw.png\" style=\"width: 600px;\"/>\n",
    "<img src=\"figs/6-softmax2.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Then, we need to use the **cross-entropy** to measure how good the predictions are, i.e., the distance between what the network tells us and what we know to be the truth. The cross-entropy is a function of weights, biases, pixels of the training image and its known label. If we compute the partial derivatives of the cross-entropy relatively to all the weights and all the biases, we obtain a **gradient**, computed for a given image, label and present value of weights and biases. We can update weights and biases by a fraction of the gradient and do the same thing again using the next batch of training images.\n",
    "<img src=\"figs/7-cross_entropy.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables and Placeholders\n",
    "First we define TensorFlow **variables** and **placeholders**. *Variables* are all the parameters that you want the training algorithm to determine for you (e.g., weights and biases). *Placeholders* are parameters that will be filled with actual data during training (e.g., training images). The shape of the tensor holding the training images is [None, 28, 28, 1] which stands for:\n",
    "  - 28, 28, 1: our images are 28x28 (784) pixels x 1 value per pixel (grayscale). The last number would be 3 for color images and is not really necessary here.\n",
    "  - None: this dimension will be the number of images in the mini-batch. It will be known at training time.\n",
    "\n",
    "We also need an additional placeholder for the training labels that will be provided alongside training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# neural network with 1 layer of 10 softmax neurons\n",
    "#\n",
    "# · · · · · · · · · ·       (input data, flattened pixels)       X [batch, 784] \n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/    -- fully connected layer (softmax)      W [784, 10]     b[10]\n",
    "#   · · · · · · · ·                                              Y_hat [batch, 10]\n",
    "\n",
    "\n",
    "# input X, place holder for image data: 28x28 grayscale images, \n",
    "# the first dimension (None) will index the images in the mini-batch, \n",
    "# the last dimension (1) is the number of channels\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct labels will go here\n",
    "# the last dimension (10) is the number of neurons \n",
    "Y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "# Weights W[784, 10], 784 = 28 * 28\n",
    "# Selects random numbers from a normal distribution whose mean or std is close to 0 and values are close to normal distribution with specified mean and standard deviation\n",
    "# In ML, it is desired to have weights close to 0\n",
    "W = tf.Variable(tf.truncated_normal([784, 10],stddev=0.1))\n",
    "\n",
    "# biases b[10]\n",
    "b = tf.Variable(tf.constant(0.1,shape=[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Model\n",
    "Now, we can make a **model** for a one-layer neural network. The formula is the one we explained before, i.e., $\\hat{Y} = softmax(X . W + b)$. You can use the `tf.nn.softmax` and `tf.matmul` to build the model. Here, we need to use the `tf.reshape` to transform our 28x28 images into single vectors of 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the 28*28 images into a single line/vector of 784 pixels\n",
    "# -1 means no learning rate, images are 28x28 (784) pixels x 1 value per pixel (grayscale) or channels. \n",
    "# The last number would be 3 for color images and is not really necessary here.\n",
    "# What happens if images are of varied dimensions e.g 27 by 30, how to convert them into 1 vector?\n",
    "flattened_X = tf.reshape(X, [-1,28*28*1])\n",
    "# The model\n",
    "Y_hat = tf.nn.softmax(tf.matmul(flattened_X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Cost Function\n",
    "Now, we have model predictions $\\hat{Y}$ and correct labels $Y$, so for each instance $i$ (image) we can compute the cross-entropy as the **cost function**: $cross\\_entropy = -\\sum(Y_i * log(\\hat{Y}i))$. You can use `reduce_mean` to add all the components in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_5:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Mean_6:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_hat)))\n",
    "cross_entropy = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels= Y, logits=Y_hat))\n",
    "print(cross_entropy)\n",
    "\n",
    "#Add components to tensor\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "print(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traine the Model\n",
    "Now, select the gradient descent optimiser `GradientDescentOptimizer` and ask it to minimise the cross-entropy cost. In this step, TensorFlow computes the partial derivatives of the cost function relatively to all the weights and all the biases (the gradient). The gradient is then used to update the weights and biases. Set the learning rate is $0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.005)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Model\n",
    "It is time to run the training loop. All the TensorFlow instructions up to this point have been preparing a computation graph in memory but nothing has been computed yet. The computation requires actual data to be fed into the placeholders. This is supplied in the form of a Python dictionary, where the keys are the names of the placeholders. During the trainig print out the cost every 200 steps. Moreove, after training the model, print out the accurray of the model by testing it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 has a loss of  2.290168\n",
      "Epoch  200 has a loss of  2.2961574\n",
      "Epoch  400 has a loss of  2.2897348\n",
      "Epoch  600 has a loss of  2.2727816\n",
      "Epoch  800 has a loss of  2.2614317\n",
      "Epoch  1000 has a loss of  2.2513862\n",
      "Epoch  1200 has a loss of  2.2192545\n",
      "Epoch  1400 has a loss of  2.1954172\n",
      "Epoch  1600 has a loss of  2.1516714\n",
      "Epoch  1800 has a loss of  2.1395924\n",
      "Epoch  2000 has a loss of  2.0917978\n",
      "Epoch  2200 has a loss of  2.097378\n",
      "Epoch  2400 has a loss of  2.1936624\n",
      "Epoch  2600 has a loss of  2.0439634\n",
      "Epoch  2800 has a loss of  2.076366\n",
      "Epoch  3000 has a loss of  2.05493\n",
      "Epoch  3200 has a loss of  2.071522\n",
      "Epoch  3400 has a loss of  2.0187588\n",
      "Epoch  3600 has a loss of  2.006071\n",
      "Epoch  3800 has a loss of  1.9985417\n",
      "Epoch  4000 has a loss of  2.0448534\n",
      "Epoch  4200 has a loss of  2.07711\n",
      "Epoch  4400 has a loss of  2.025236\n",
      "Epoch  4600 has a loss of  2.0304399\n",
      "Epoch  4800 has a loss of  2.013119\n",
      " \n",
      "The Accuracy for the model in One-Layer Neural Network model is: 54.04000282287598 % \n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "save_accuracy = []\n",
    "n_epochs = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist.train.next_batch(100)\n",
    "        #Error: Cannot feed value of shape (200, 784) for Tensor u'Placeholder_18:0', which has shape '(?, 28, 28, 1)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step,cross_entropy], feed_dict={X: epoch_imageX_reshapedbatch, Y: epoch_labelY_batch})\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        #print loss after every \n",
    "        if epoch % 200 == 0:\n",
    "            print('Epoch ',epoch, 'has a loss of ', loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_hat,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X:epoch_imageX_reshapedTestbatch, Y: mnist.test.labels}) * 100\n",
    "    print(\" \")\n",
    "    print(\"The Accuracy for the model in One-Layer Neural Network model is: {} % \".format(acc) )\n",
    "\n",
    "\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Add More Layers\n",
    "\n",
    "<img src=\"figs/8-comic2.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Now, let's improve the recognition accuracy by adding more layers to the neural network. The neurons in the second layer, instead of computing weighted sums of pixels will compute weighted sums of neuron outputs from the previous layer. We keep the softmax function as the activation function on the last layer, but on intermediate layers we will use the the **sigmoid** activation function. So, let's build a five-layer fully connected neural network with the following structure, and train the model with the trainging data and print out its accuracy on the test data.\n",
    "<img src=\"figs/9-five_layer.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_ml = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five layers and their number of neurons, i.e., 200, 100, 60, 30, and 10\n",
    "The Size of the filter/kernel is 5x5; Input channels is 1 (grayscale), 3 for coloured pics \n",
    "200 different feature maps meaning 32 different filters are applied on each image.\n",
    "he output volume size of Conv1 would be 28x28x200). \n",
    "Filter / kernel tensor is of shape [filter_height, filter_width, in_channels, out_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# neural network with five layers\n",
    "#\n",
    "# · · · · · · · · · ·          (input data, flattened pixels)       X [batch, 784]   \n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/       -- fully connected layer (sigmoid)      W1 [784, 200] B1 [200]\n",
    "\n",
    "#  · · · · · · · · ·                                                Y1_hat [batch, 200]\n",
    "#   \\x/x\\x/x\\x/x\\x/         -- fully connected layer (sigmoid)      W2 [200, 100]      B2 [100]\n",
    "\n",
    "#    · · · · · · ·                                                  Y2_hat [batch, 100]\n",
    "#     \\x/x\\x/x\\x/           -- fully connected layer (sigmoid)      W3 [100, 60]       B3 [60]\n",
    "\n",
    "#      · · · · ·                                                    Y3_hat [batch, 60]\n",
    "#       \\x/x\\x/             -- fully connected layer (sigmoid)      W4 [60, 30]        B4 [30]\n",
    "\n",
    "#        · · ·                                                      Y4_hat [batch, 30]\n",
    "#         \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5 [10]\n",
    "#          ·                                                        Y_hat [batch, 10]\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "\n",
    "# ML(Multi layer)\n",
    "X_ML = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ML = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "# First Conv Layer with 28*28 input features and 200 ouput features\n",
    "# W_Conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 200],stddev=0.1)) why doesn't work??\n",
    "# Dimension 1 in both shapes must be equal, but are 28 and 5. \n",
    "# Shapes are [?,28] and [5,5]. for 'MatMul_5' (op: 'BatchMatMul') with input shapes: [?,28,28,1], [5,5,1,200].\n",
    "W_Conv1 = tf.Variable(tf.truncated_normal([784, 200],stddev=0.1))\n",
    "B_Conv1 = tf.Variable(tf.constant(0.1, shape=[200]))\n",
    "\n",
    "# Second Conv Layer with 200 input features and 100 ouput features\n",
    "W_Conv2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "B_Conv2 = tf.Variable(tf.constant(0.1, shape=[100]))\n",
    "\n",
    "# Third Conv Layer with 100 input features and 100 ouput features\n",
    "W_Conv3 = tf.Variable(tf.truncated_normal([100, 60], stddev=0.1))\n",
    "B_Conv3 = tf.Variable(tf.constant(0.1, shape=[60]))\n",
    "\n",
    "# Fourth Conv Layer with 60 input features and 30 ouput features\n",
    "W_Conv4 = tf.Variable(tf.truncated_normal([60, 30], stddev=0.1))\n",
    "B_Conv4 = tf.Variable(tf.constant(0.1, shape=[30]))\n",
    "\n",
    "#Fourth Conv Layer with 30 input features and 10 ouput features\n",
    "W_Conv5 = tf.Variable(tf.truncated_normal([30, 10], stddev=0.1))\n",
    "B_Conv5 = tf.Variable(tf.constant(0.1, shape=[10])) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_Conv1 <tf.Variable 'Variable:0' shape=(784, 200) dtype=float32_ref>\n",
      "flattened_X_ML <tf.Variable 'Variable:0' shape=(784, 200) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "\n",
    "#Flatten 28*28 images into a single line/vector of 784 pixels, 1 represents grayscale\n",
    "flattened_X_ML = tf.reshape(X_ML, [-1, 28*28*1])\n",
    "print (\"W_Conv1\", W_Conv1)\n",
    "print (\"flattened_X_ML\", W_Conv1)\n",
    "Y_hat_Conv1 = tf.nn.sigmoid(tf.matmul(flattened_X_ML, W_Conv1) + B_Conv1)\n",
    "Y_hat_Conv2 = tf.nn.sigmoid(tf.matmul(Y_hat_Conv1, W_Conv2) + B_Conv2)\n",
    "Y_hat_Conv3 = tf.nn.sigmoid(tf.matmul(Y_hat_Conv2, W_Conv3) + B_Conv3)\n",
    "Y_hat_Conv4 = tf.nn.sigmoid(tf.matmul(Y_hat_Conv3, W_Conv4) + B_Conv4)\n",
    "Y_hat_ML = tf.nn.softmax(tf.matmul(Y_hat_Conv4, W_Conv5) + B_Conv5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "Epoch  0 has a loss of  2.3039727210998535\n",
      "Epoch  200 has a loss of  2.307284355163574\n",
      "Epoch  400 has a loss of  2.3060741424560547\n",
      "Epoch  600 has a loss of  2.306739568710327\n",
      "Epoch  800 has a loss of  2.298088312149048\n",
      "Epoch  1000 has a loss of  2.298781156539917\n",
      "Epoch  1200 has a loss of  2.301783561706543\n",
      "Epoch  1400 has a loss of  2.3015966415405273\n",
      "Epoch  1600 has a loss of  2.297822952270508\n",
      "Epoch  1800 has a loss of  2.302208185195923\n",
      "Epoch  2000 has a loss of  2.3043437004089355\n",
      "Epoch  2200 has a loss of  2.3037538528442383\n",
      "Epoch  2400 has a loss of  2.3015990257263184\n",
      "Epoch  2600 has a loss of  2.304655075073242\n",
      "Epoch  2800 has a loss of  2.3003900051116943\n",
      "Epoch  3000 has a loss of  2.299161911010742\n",
      "Epoch  3200 has a loss of  2.3031258583068848\n",
      "Epoch  3400 has a loss of  2.3017220497131348\n",
      "Epoch  3600 has a loss of  2.3023767471313477\n",
      "Epoch  3800 has a loss of  2.303291082382202\n",
      "Epoch  4000 has a loss of  2.3080272674560547\n",
      "Epoch  4200 has a loss of  2.3008527755737305\n",
      "Epoch  4400 has a loss of  2.296020984649658\n",
      "Epoch  4600 has a loss of  2.3027451038360596\n",
      "Epoch  4800 has a loss of  2.3029613494873047\n",
      " \n",
      "The Accuracy for the model in Five-Layer Neural Network model is: 11.349999904632568 % \n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# define the cost function\n",
    "########################################\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_hat)))\n",
    "\n",
    "cross_entropy_ml = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels= Y_ML, logits=Y_hat_ML))\n",
    "print(cross_entropy_ml)\n",
    "\n",
    "#Add components to tensor\n",
    "cross_entropy_ml = tf.reduce_mean(cross_entropy_ml)\n",
    "print(cross_entropy_ml)\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.005)\n",
    "train_step_ml = optimizer.minimize(cross_entropy_ml)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist_ml.train.next_batch(100)\n",
    "        #Error: Cannot feed value of shape (200, 784) for Tensor u'Placeholder_18:0', which has shape '(?, 28, 28, 1)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step_ml,cross_entropy_ml], feed_dict={X_ML: epoch_imageX_reshapedbatch, Y_ML: epoch_labelY_batch})\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        #print loss after every \n",
    "        if epoch % 200 == 0:\n",
    "            print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y_ML, 1), tf.argmax(Y_hat_ML,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist_ml.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X_ML:epoch_imageX_reshapedTestbatch, Y_ML: mnist_ml.test.labels}) * 100\n",
    "    print(\" \")\n",
    "    print(\"The Accuracy for the model in Five-Layer Neural Network model is: {} % \".format(acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Special Care for Deep Networks\n",
    "As layers were added, neural networks tended to converge with more difficulties. For example, the accuracy could stuck at 0.1. Here, we want to apply some updates to the network we built in the previous part to improve its performance. \n",
    "\n",
    "### ReLU Activation Function\n",
    "<img src=\"figs/10-comic3.png\" style=\"width: 500px;\"/>\n",
    "The sigmoid activation function is actually quite problematic in deep networks. It squashes all values between 0 and 1 and when you do so repeatedly, neuron outputs and their gradients can vanish entirely. An alternative activation function is **ReLU** that shows better performance compare to sigmoid. It looks like as below:\n",
    "<img src=\"figs/11-relu.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "### A Better Optimizer\n",
    "In very high dimensional spaces like here, **saddle points** are frequent. These are points that are not local minima, but where the gradient is nevertheless zero and the gradient descent optimizer stays stuck there. One possible solution to tackle this probelm is to use better optimizers, such as Adam optimizer `tf.train.AdamOptimizer`.\n",
    "\n",
    "### Random Initialisations\n",
    "When working with ReLUs, the best practice is to initialise bias values to small positive values, so that neurons operate in the non-zero range of the ReLU initially.\n",
    "\n",
    "### Learning Rate\n",
    "<img src=\"figs/12-comic4.png\" style=\"width: 500px;\"/>\n",
    "With two, three or four intermediate layers, you can now get close to 98% accuracy, if you push the iterations to 5000 or beyond. But, the results are not very consistent, and the curves jump up and down by a whole percent. A good solution is to start fast and decay the learning rate exponentially from $0.005$ to $0.0001$ for example. In order to pass a different learning rate to the `AdamOptimizer` at each iteration, you will need to define a new placeholder and feed it a new value at each iteration through `feed_dict`. Here is the formula for exponential decay: $learning\\_rate = lr\\_min + (lr\\_max - lr\\_min) * e^{\\frac{-i}{2000}}$, where $i$ is the iteration number.\n",
    "\n",
    "### NaN?\n",
    "In the network you built in the last section, you might see accuracy curve crashes and the console outputs NaN for the cross-entropy. It may happen, because you are attempting to compute a $log(0)$, which is indeed Not A Number (NaN). Remember that the cross-entropy involves a log, computed on the output of the softmax layer. Since softmax is essentially an exponential, which is never zero, we should be fine, but with 32 bit precision floating-point operations, exp(-100) is already a genuine zero. TensorFlow has a handy function that computes the softmax and the cross-entropy in a single step, implemented in a numerically stable way. To use it, you will need to separate the weighted sum plus bias on the last layer, before softmax is applied and then give it with the true values to the function `tf.nn.softmax_cross_entropy_with_logits`.\n",
    "\n",
    "In the code below, apply the following changes and show their impact on the accuracy of the model on training data, as well as the test data:\n",
    "* Replace the sigmoid activation function with ReLU\n",
    "* Use the Adam optimizer\n",
    "* Initialize weights with small random values between -0.2 and +0.2, and make sure biases are initialised with small positive values, for example 0.1\n",
    "* Update the learning rate in different iterations. Start fast and decay the learning rate exponentially from $0.005$ to $0.0001$, i.e., \n",
    "```\n",
    "max_learning_rate = 0.005\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 2000.0\n",
    "```\n",
    "* Use `tf.nn.softmax_cross_entropy_with_logits` to prevent getting NaN in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 has a loss of  229.76202392578125\n",
      "Epoch  200 has a loss of  163.44586181640625\n",
      "Epoch  400 has a loss of  160.47909545898438\n",
      "Epoch  600 has a loss of  160.53448486328125\n",
      "Epoch  800 has a loss of  157.03350830078125\n",
      "Epoch  1000 has a loss of  159.6430206298828\n",
      "Epoch  1200 has a loss of  164.748046875\n",
      "Epoch  1400 has a loss of  159.29676818847656\n",
      "Epoch  1600 has a loss of  160.05653381347656\n",
      "Epoch  1800 has a loss of  159.0186004638672\n",
      "Epoch  2000 has a loss of  165.7208709716797\n",
      "Epoch  2200 has a loss of  157.09127807617188\n",
      "Epoch  2400 has a loss of  155.005126953125\n",
      "Epoch  2600 has a loss of  163.22683715820312\n",
      "Epoch  2800 has a loss of  151.09738159179688\n",
      "Epoch  3000 has a loss of  148.61399841308594\n",
      "Epoch  3200 has a loss of  150.1233673095703\n",
      "Epoch  3400 has a loss of  149.11695861816406\n",
      "Epoch  3600 has a loss of  146.13360595703125\n",
      "Epoch  3800 has a loss of  147.90016174316406\n",
      "Epoch  4000 has a loss of  147.92959594726562\n",
      "Epoch  4200 has a loss of  148.68624877929688\n",
      "Epoch  4400 has a loss of  147.7989044189453\n",
      "Epoch  4600 has a loss of  149.46170043945312\n",
      "Epoch  4800 has a loss of  148.09849548339844\n",
      " \n",
      "The Accuracy for an fine tuned model in Five-Layer Neural Network model is: 96.8999981880188 % \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# neural network with 5 layers\n",
    "#\n",
    "# · · · · · · · · · ·          (input data, flattened pixels)       X [batch, 784]   \n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/       -- fully connected layer (sigmoid)      W1 [784, 200]      B1[200]\n",
    "#  · · · · · · · · ·                                                Y1_hat [batch, 200]\n",
    "#   \\x/x\\x/x\\x/x\\x/         -- fully connected layer (sigmoid)      W2 [200, 100]      B2[100]\n",
    "#    · · · · · · ·                                                  Y2_hat [batch, 100]\n",
    "#     \\x/x\\x/x\\x/           -- fully connected layer (sigmoid)      W3 [100, 60]       B3[60]\n",
    "#      · · · · ·                                                    Y3_hat [batch, 60]\n",
    "#       \\x/x\\x/             -- fully connected layer (sigmoid)      W4 [60, 30]        B4[30]\n",
    "#        · · ·                                                      Y4_hat [batch, 30]\n",
    "#         \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]\n",
    "#          ·                                                        Y_hat [batch, 10]\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "X_SC = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_SC = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "\n",
    "\n",
    "# five layers and their number of neurons, i.e., 200, 100, 60, 30, and 10\n",
    "# when using RELUs, make sure biases are initialised with small positive values, for example 0.1\n",
    "W_Conv1 = tf.Variable(tf.truncated_normal([784, 200],stddev=0.1))\n",
    "B_Conv1 = tf.Variable(tf.constant(0.1, shape=[200]))\n",
    "\n",
    "# Second Conv Layer with 200 input features and 100 ouput features\n",
    "W_Conv2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "B_Conv2 = tf.Variable(tf.constant(0.1, shape=[100]))\n",
    "\n",
    "# Third Conv Layer with 100 input features and 100 ouput features\n",
    "W_Conv3 = tf.Variable(tf.truncated_normal([100, 60], stddev=0.1))\n",
    "B_Conv3 = tf.Variable(tf.constant(0.1, shape=[60]))\n",
    "\n",
    "# Fourth Conv Layer with 60 input features and 30 ouput features\n",
    "W_Conv4 = tf.Variable(tf.truncated_normal([60, 30], stddev=0.1))\n",
    "B_Conv4 = tf.Variable(tf.constant(0.1, shape=[30]))\n",
    "\n",
    "#Fourth Conv Layer with 30 input features and 10 ouput features\n",
    "W_Conv5 = tf.Variable(tf.truncated_normal([30, 10], stddev=0.1))\n",
    "B_Conv5 = tf.Variable(tf.constant(0.1, shape=[10])) \n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "\n",
    "flattened_X_SC = tf.reshape(X_SC, [-1, 28*28*1])\n",
    "## ReLu goes through all outputs in Conv layer, wherever a negative number occurs, we swap it out for a 0\n",
    "Y_hat_Conv1 = tf.nn.relu(tf.matmul(flattened_X_SC, W_Conv1) + B_Conv1)\n",
    "Y_hat_Conv2 = tf.nn.relu(tf.matmul(Y_hat_Conv1, W_Conv2) + B_Conv2)\n",
    "Y_hat_Conv3 = tf.nn.relu(tf.matmul(Y_hat_Conv2, W_Conv3) + B_Conv3)\n",
    "Y_hat_Conv4 = tf.nn.relu(tf.matmul(Y_hat_Conv3, W_Conv4) + B_Conv4)\n",
    "Y_hat_SC = tf.nn.softmax(tf.matmul(Y_hat_Conv4, W_Conv5) + B_Conv5)\n",
    "\n",
    "########################################\n",
    "# defining the cost function\n",
    "########################################\n",
    "cross_entropy_sc = tf.nn.softmax_cross_entropy_with_logits(logits=Y_hat_SC, labels=Y_SC)\n",
    "cross_entropy_sc = tf.reduce_mean(cross_entropy_sc) * 100\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "decay_rate = 2000.0 #0.96\n",
    "#global_steps = 1000\n",
    "min_learning_rate = 0.0001\n",
    "max_learning_rate = 0.005 #decays exponentially at every training step\n",
    "decay_steps = 100\n",
    "current_global_step = tf.Variable(tf.constant(0))\n",
    "# Variable learning rate\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step_sc = optimizer.minimize(cross_entropy_sc)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "not_minimum_learning_rate = True\n",
    "applied_learning_rate= max_learning_rate\n",
    "n_epochs = 5000\n",
    "import math\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist_ml.train.next_batch(100)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step_sc,cross_entropy_sc], feed_dict={X_SC: epoch_imageX_reshapedbatch, \n",
    "                                                                             Y_SC: epoch_labelY_batch, \n",
    "                                                                             learning_rate: applied_learning_rate}) \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if not_minimum_learning_rate:\n",
    "            applied_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / 2000)) #Pattern Recognition and Computer Vision: First Chinese Conference pg 401\n",
    "            if applied_learning_rate == min_learning_rate:\n",
    "                not_minimum_learning_rate = False         \n",
    "        #print loss after every 200 epochs\n",
    "        if epoch % 200 == 0:\n",
    "            print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y_SC, 1), tf.argmax(Y_hat_SC,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist_ml.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X_SC:epoch_imageX_reshapedTestbatch, Y_SC: mnist_ml.test.labels}) * 100\n",
    "    print(\" \")\n",
    "    print(\"The Accuracy for an fine tuned model in Five-Layer Neural Network model is: {} % \".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "                                           \n",
    "#### Tensorflow Inbult function gives bad performance when used with AdamOptimizer\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100000, 0.96, staircase=True)\n",
    "                                           \n",
    " global_step = tf.Variable(0, trainable=False)\n",
    " starter_learning_rate = 0.1\n",
    " learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                             global_step,\n",
    "                                             100, 0.9, staircase=True)\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "Epoch  0 has a loss of  233.9381866455078\n",
    "Epoch  200 has a loss of  232.11508178710938\n",
    "Epoch  400 has a loss of  238.1150665283203\n",
    "Epoch  600 has a loss of  242.11509704589844\n",
    "Epoch  800 has a loss of  240.1150665283203\n",
    "Epoch  1000 has a loss of  236.11508178710938\n",
    "Epoch  1200 has a loss of  240.1150665283203\n",
    "Epoch  1400 has a loss of  nan\n",
    "Epoch  1600 has a loss of  nan\n",
    "Epoch  1800 has a loss of  nan\n",
    "Epoch  2000 has a loss of  nan\n",
    "Epoch  2200 has a loss of  nan\n",
    "Epoch  2400 has a loss of  nan\n",
    "Epoch  2600 has a loss of  nan\n",
    "Epoch  2800 has a loss of  nan\n",
    "Epoch  3000 has a loss of  nan\n",
    "Epoch  3200 has a loss of  nan\n",
    "Epoch  3400 has a loss of  nan\n",
    "Epoch  3600 has a loss of  nan\n",
    "Epoch  3800 has a loss of  nan\n",
    "Epoch  4000 has a loss of  nan\n",
    "Epoch  4200 has a loss of  nan\n",
    "Epoch  4400 has a loss of  nan\n",
    "Epoch  4600 has a loss of  nan\n",
    "Epoch  4800 has a loss of  nan\n",
    " \n",
    "The Accuracy for an fine tuned model in Five-Layer Neural Network model is: 9.799999743700027 % \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Overfitting and Dropout\n",
    "<img src=\"figs/13-comic5.png\" style=\"width: 500px;\"/>\n",
    "You will have noticed that cross-entropy curves for test and training data start disconnecting after a couple thousand iterations. The learning algorithm works on training data only and optimises the training cross-entropy accordingly. It never sees test data so it is not surprising that after a while its work no longer has an effect on the test cross-entropy which stops dropping and sometimes even bounces back up. \n",
    "<img src=\"figs/14-overfit.png\" style=\"width: 500px;\"/>\n",
    "This disconnect is usually labeled **overfitting** and when you see it, you can try to apply a regularisation technique called **dropout**. In dropout, at each training iteration, you drop random neurons from the network. You choose a probability `pkeep` for a neuron to be kept, usually between 50% and 75%, and then at each iteration of the training loop, you randomly remove neurons with all their weights and biases. Different neurons will be dropped at each iteration. When testing the performance of your network of course you put all the neurons back (`pkeep = 1`).\n",
    "<img src=\"figs/15-dropout.png\" style=\"width: 500px;\"/>\n",
    "TensorFlow offers a dropout function to be used on the outputs of a layer of neurons. It randomly zeroes-out some of the outputs and boosts the remaining ones by `1 / pkeep`. You can add dropout after each intermediate layer in the network now. \n",
    "\n",
    "In the following code, use the dropout between each layer during the training, and set the probability `pkeep` once to $50%$ and another time to $75%$ and compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7192a17bd9da>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "mnist_do = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DropOut Rate  0.5\n",
      "Epoch  0 has a loss of  230.99819946289062\n",
      "Epoch  200 has a loss of  177.94859313964844\n",
      "Epoch  400 has a loss of  177.68589782714844\n",
      "Epoch  600 has a loss of  171.83956909179688\n",
      "Epoch  800 has a loss of  171.4212646484375\n",
      "Epoch  1000 has a loss of  171.4695281982422\n",
      "Epoch  1200 has a loss of  166.45745849609375\n",
      "Epoch  1400 has a loss of  165.58737182617188\n",
      "Epoch  1600 has a loss of  160.1772918701172\n",
      "Epoch  1800 has a loss of  162.64515686035156\n",
      "Epoch  2000 has a loss of  161.71612548828125\n",
      "Epoch  2200 has a loss of  163.24974060058594\n",
      "Epoch  2400 has a loss of  158.32669067382812\n",
      "Epoch  2600 has a loss of  162.86239624023438\n",
      "Epoch  2800 has a loss of  159.2679901123047\n",
      "Epoch  3000 has a loss of  154.70919799804688\n",
      "Epoch  3200 has a loss of  161.9462432861328\n",
      "Epoch  3400 has a loss of  160.61312866210938\n",
      "Epoch  3600 has a loss of  160.8137969970703\n",
      "Epoch  3800 has a loss of  161.47116088867188\n",
      "Epoch  4000 has a loss of  156.34494018554688\n",
      "Epoch  4200 has a loss of  152.48516845703125\n",
      "Epoch  4400 has a loss of  161.1444549560547\n",
      "Epoch  4600 has a loss of  159.82052612304688\n",
      "Epoch  4800 has a loss of  162.23902893066406\n",
      " DropOut Rate  0.7\n",
      "Epoch  0 has a loss of  230.2528076171875\n",
      "Epoch  200 has a loss of  162.80743408203125\n",
      "Epoch  400 has a loss of  158.9005584716797\n",
      "Epoch  600 has a loss of  156.17184448242188\n",
      "Epoch  800 has a loss of  155.16552734375\n",
      "Epoch  1000 has a loss of  158.04989624023438\n",
      "Epoch  1200 has a loss of  155.64111328125\n",
      "Epoch  1400 has a loss of  155.10183715820312\n",
      "Epoch  1600 has a loss of  154.05419921875\n",
      "Epoch  1800 has a loss of  158.85720825195312\n",
      "Epoch  2000 has a loss of  154.1165313720703\n",
      "Epoch  2200 has a loss of  156.12136840820312\n",
      "Epoch  2400 has a loss of  155.1880340576172\n",
      "Epoch  2600 has a loss of  158.55201721191406\n",
      "Epoch  2800 has a loss of  153.22283935546875\n",
      "Epoch  3000 has a loss of  149.97511291503906\n",
      "Epoch  3200 has a loss of  153.12075805664062\n",
      "Epoch  3400 has a loss of  153.10780334472656\n",
      "Epoch  3600 has a loss of  151.02081298828125\n",
      "Epoch  3800 has a loss of  153.3347625732422\n",
      "Epoch  4000 has a loss of  154.11520385742188\n",
      "Epoch  4200 has a loss of  150.6924285888672\n",
      "Epoch  4400 has a loss of  156.05203247070312\n",
      "Epoch  4600 has a loss of  152.361572265625\n",
      "Epoch  4800 has a loss of  156.89361572265625\n",
      " \n",
      "The Accuracy for 5-Layer Neural Network Model with Dropout is: [88.7000024318695, 93.94999742507935] % \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# neural network with 5 layers\n",
    "#\n",
    "# · · · · · · · · · ·          (input data, flattened pixels)       X [batch, 784]   \n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/       -- fully connected layer (sigmoid)      W1 [784, 200]      B1[200]\n",
    "#  · · · · · · · · ·                                                Y1_hat [batch, 200]\n",
    "#   \\x/x\\x/x\\x/x\\x/         -- fully connected layer (sigmoid)      W2 [200, 100]      B2[100]\n",
    "#    · · · · · · ·                                                  Y2_hat [batch, 100]\n",
    "#     \\x/x\\x/x\\x/           -- fully connected layer (sigmoid)      W3 [100, 60]       B3[60]\n",
    "#      · · · · ·                                                    Y3_hat [batch, 60]\n",
    "#       \\x/x\\x/             -- fully connected layer (sigmoid)      W4 [60, 30]        B4[30]\n",
    "#        · · ·                                                      Y4_hat [batch, 30]\n",
    "#         \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]\n",
    "#          ·                                                        Y_hat [batch, 10]\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "X_DO = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_DO = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "# variable learning rate\n",
    "learning_rate_do = tf.placeholder(tf.float32)\n",
    "\n",
    "# probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "# five layers and their number of neurons, i.e., 200, 100, 60, 30, and 10\n",
    "# when using RELUs, make sure biases are initialised with small positive values, for example 0.1\n",
    "W_Conv1 = tf.Variable(tf.truncated_normal([784, 200],stddev=0.1))\n",
    "B_Conv1 = tf.Variable(tf.constant(0.1, shape=[200]))\n",
    "\n",
    "# Second Conv Layer with 200 input features and 100 ouput features\n",
    "W_Conv2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "B_Conv2 = tf.Variable(tf.constant(0.1, shape=[100]))\n",
    "\n",
    "# Third Conv Layer with 100 input features and 100 ouput features\n",
    "W_Conv3 = tf.Variable(tf.truncated_normal([100, 60], stddev=0.1))\n",
    "B_Conv3 = tf.Variable(tf.constant(0.1, shape=[60]))\n",
    "\n",
    "# Fourth Conv Layer with 60 input features and 30 ouput features\n",
    "W_Conv4 = tf.Variable(tf.truncated_normal([60, 30], stddev=0.1))\n",
    "B_Conv4 = tf.Variable(tf.constant(0.1, shape=[30]))\n",
    "\n",
    "#Fourth Conv Layer with 30 input features and 10 ouput features\n",
    "W_Conv5 = tf.Variable(tf.truncated_normal([30, 10], stddev=0.1))\n",
    "B_Conv5 = tf.Variable(tf.constant(0.1, shape=[10])) \n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "flattened_X_DO = tf.reshape(X_DO, [-1,28*28*1])\n",
    "\n",
    "Y_hat_Conv1 = tf.nn.relu(tf.matmul(flattened_X_DO, W_Conv1) + B_Conv1)\n",
    "Y1_hat_Conv1_dropout = tf.nn.dropout(Y_hat_Conv1, keep_prob=pkeep)\n",
    "\n",
    "Y_hat_Conv2 = tf.nn.relu(tf.matmul(Y1_hat_Conv1_dropout, W_Conv2) + B_Conv2)\n",
    "Y_hat_Conv2_dropout = tf.nn.dropout(Y_hat_Conv2, keep_prob=pkeep)\n",
    "\n",
    "Y_hat_Conv3 = tf.nn.relu(tf.matmul(Y_hat_Conv2_dropout, W_Conv3) + B_Conv3)\n",
    "Y_hat_Conv3_dropout = tf.nn.dropout(Y_hat_Conv3, keep_prob=pkeep)\n",
    "\n",
    "Y_hat_Conv4 = tf.nn.relu(tf.matmul(Y_hat_Conv3_dropout, W_Conv4) + B_Conv4)\n",
    "Y_hat_Conv4_dropout = tf.nn.dropout(Y_hat_Conv4, keep_prob=pkeep)\n",
    "\n",
    "Y_hat_DO = tf.nn.softmax(tf.matmul(Y_hat_Conv4_dropout, W_Conv5) + B_Conv5)\n",
    "\n",
    "\n",
    "########################################\n",
    "# define the cost function\n",
    "########################################\n",
    "cross_entropy_do = tf.nn.softmax_cross_entropy_with_logits(logits=Y_hat_DO, labels=Y_DO)\n",
    "cross_entropy_do = tf.reduce_mean(cross_entropy_do) * 100\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "decay_speed = 2000\n",
    "min_learning_rate = 0.0001\n",
    "max_learning_rate = 0.005 #decays exponentially at every training step\n",
    "current_global_step = tf.Variable(tf.constant(0))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate_do)\n",
    "train_step_do = optimizer.minimize(cross_entropy_do)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "dropout_rates = [0.5,0.7]\n",
    "def execute_model(dropout_rate):  \n",
    "    not_minimum_learning_rate = True\n",
    "    applied_learning_rate = max_learning_rate\n",
    "    print(\" DropOut Rate \",dropout_rate)\n",
    "    n_epochs = 5000\n",
    "    import math\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        #For every iteration i\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_imageX_batch, epoch_labelY_batch = mnist_do.train.next_batch(100)\n",
    "            epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "\n",
    "            _, loss = sess.run([train_step_do,cross_entropy_do], feed_dict={X_DO: epoch_imageX_reshapedbatch, \n",
    "                                                                            Y_DO: epoch_labelY_batch, \n",
    "                                                                            learning_rate_do: applied_learning_rate,\n",
    "                                                                            pkeep: dropout_rate}) \n",
    "\n",
    "            epoch_loss = epoch_loss + loss\n",
    "            if not_minimum_learning_rate:\n",
    "                applied_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / decay_speed)) #Pattern Recognition and Computer Vision: First Chinese Conference pg 401\n",
    "                if applied_learning_rate == min_learning_rate:\n",
    "                    not_minimum_learning_rate = False         \n",
    "            #print loss after every 200 epochs\n",
    "            if epoch % 200 == 0:\n",
    "                print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(Y_DO, 1), tf.argmax(Y_hat_DO,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "        epoch_imageX_reshapedTestbatch = np.reshape(mnist_do.test.images, [-1,28,28,1])\n",
    "        acc = accuracy.eval(session=sess,feed_dict={X_DO:epoch_imageX_reshapedTestbatch, \n",
    "                                                    Y_DO: mnist_do.test.labels, \n",
    "                                                    learning_rate_do: applied_learning_rate,\n",
    "                                                    pkeep: dropout_rate }) * 100\n",
    "\n",
    "        return acc\n",
    "perfomance_results = [execute_model(dropout_rate) for dropout_rate in dropout_rates]\n",
    "print(\" \")\n",
    "print(\"The Accuracy for 5-Layer Neural Network Model with Dropout is: {} % \".format(perfomance_results))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Convolutional Network\n",
    "<img src=\"figs/16-comic6.png\" style=\"width: 500px;\"/>\n",
    "In the previous sections, all pixels of images flattened into a single vector, which was a really bad idea. Handwritten digits are made of shapes and we discarded the shape information when we flattened the pixels. However, we can use **convolutional neural networks (CNN)** to take advantage of shape information. CNNs apply *a series of filters* to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. CNNs contains three components:\n",
    "  - **Convolutional layers**: apply a specified number of convolution filters to the image. For each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. Convolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\n",
    "  - **Pooling layers**: downsample the image data extracted by the convolutional layers to reduce the dimensionality of the feature map in order to decrease processing time. A commonly used pooling algorithm is max pooling, which extracts subregions of the feature map (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other values.\n",
    "  - **Dense (fully connected) layers**: perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "  \n",
    "Typically, a CNN is composed of a *stack of **convolutional modules*** that perform feature extraction. Each *module* consists of a *convolutional layer* followed by a *pooling layer*. The last convolutional module is followed by one or more dense layers that perform classification. The final dense layer in a CNN contains a single neuron for each target class in the model, with a softmax activation function to generate a value between 0-1 for each neuron. We can interpret the softmax values for a given image as relative measurements of how likely it is that the image falls into each target class.\n",
    "\n",
    "Now, let us build a convolutional network for handwritten digit recognition. In this assignment, we will use the architecture shown in the following figure that has three convolutional layers, one fully-connected layer, and one softmax layer. Notice that the second and third convolutional layers have a stride of two that explains why they bring the number of output values down from 28x28 to 14x14 and then 7x7. A convolutional layer requires a weights tensor like `[4, 4, 3, 2]`, in which the first two numbers define the size of a filter (map), the third number shows the *depth* of the filter that is the number of *input channel*, and the last number shows the number of *output channel*. The output channel defines the number of times that we repeat the same thing with a different set of weights in one layer. In our implementation, we assume the output depth of first three convolutional layers, are 4, 8, 12, and the size of fully connected layer is 200.\n",
    "<img src=\"figs/17-arch1.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Convolutional layers can be implemented in TensorFlow using the `tf.nn.conv2d` function, which performs the scanning of the input image in both directions using the supplied weights. This is only the weighted sum part of the neuron. You still need to add a bias and feed the result through an activation function. The padding strategy that works here is to copy pixels from the sides of the image. All digits are on a uniform background so this just extends the background and should not add any unwanted shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "mnist_cnn = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 has a loss of  230.14256286621094\n",
      "Epoch  200 has a loss of  172.47821044921875\n",
      "Epoch  400 has a loss of  169.77420043945312\n",
      "Epoch  600 has a loss of  160.7787322998047\n",
      "Epoch  800 has a loss of  157.1842803955078\n",
      "Epoch  1000 has a loss of  151.1025390625\n",
      "Epoch  1200 has a loss of  148.56292724609375\n",
      "Epoch  1400 has a loss of  150.09075927734375\n",
      "Epoch  1600 has a loss of  155.11459350585938\n",
      "Epoch  1800 has a loss of  150.1013641357422\n",
      "Epoch  2000 has a loss of  149.15672302246094\n",
      "Epoch  2200 has a loss of  147.86929321289062\n",
      "Epoch  2400 has a loss of  153.7568817138672\n",
      "Epoch  2600 has a loss of  146.11526489257812\n",
      "Epoch  2800 has a loss of  149.02096557617188\n",
      "Epoch  3000 has a loss of  147.13235473632812\n",
      "Epoch  3200 has a loss of  147.11509704589844\n",
      "Epoch  3400 has a loss of  149.1150665283203\n",
      "Epoch  3600 has a loss of  146.22799682617188\n",
      "Epoch  3800 has a loss of  147.1150665283203\n",
      "Epoch  4000 has a loss of  146.1150665283203\n",
      "Epoch  4200 has a loss of  149.5115966796875\n",
      "Epoch  4400 has a loss of  149.11505126953125\n",
      "Epoch  4600 has a loss of  146.14134216308594\n",
      "Epoch  4800 has a loss of  146.1150665283203\n",
      " end\n",
      "The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: 97.97999858856201 % \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# · · · · · · · · · ·      (input data, 1-deep)               X [batch, 28, 28, 1]\n",
    "# @ @ @ @ @ @ @ @ @ @   -- conv. layer 5x5x1=>4 stride 1      W1 [5, 5, 1, 4]        B1 [4]\n",
    "# ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                         Y1_hat [batch, 28, 28, 4]\n",
    "#   @ @ @ @ @ @ @ @     -- conv. layer 5x5x4=>8 stride 2      W2 [5, 5, 4, 8]        B2 [8]\n",
    "#   ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                           Y2_hat [batch, 14, 14, 8]\n",
    "#     @ @ @ @ @ @       -- conv. layer 4x4x8=>12 stride 2     W3 [4, 4, 8, 12]       B3 [12]\n",
    "#     ∶∶∶∶∶∶∶∶∶∶∶                                             Y3_hat [batch, 7, 7, 12] => reshaped to YY [batch, 7*7*12]\n",
    "#      \\x/x\\x\\x/        -- fully connected layer (relu)       W4 [7*7*12, 200]       B4 [200]\n",
    "#       · · · ·                                               Y4_hat [batch, 200]\n",
    "#       \\x/x\\x/         -- fully connected layer (softmax)    W5 [200, 10]           B5 [10]\n",
    "#        · · ·                                                Y_hat [batch, 10]\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "# ML(Multi layer)\n",
    "X_CNN = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_CNN = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "learning_rate_cnn = tf.placeholder(tf.float32)\n",
    "\n",
    "# three convolutional layers with their channel counts, and a fully connected layer \n",
    "# (the last layer has 10 softmax neurons)\n",
    "# the output depth of first three convolutional layers, are 4, 8, 12, and the size of fully connected\n",
    "# layer is 200\n",
    "W_Conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 4],stddev=0.1))\n",
    "B_Conv1 = tf.Variable(tf.constant(0.1, shape=[4]))\n",
    "\n",
    "W_Conv2 = tf.Variable(tf.truncated_normal([5, 5, 4, 8],stddev=0.1))\n",
    "B_Conv2 = tf.Variable(tf.constant(0.1, shape=[8]))\n",
    "\n",
    "W_Conv3 = tf.Variable(tf.truncated_normal([4, 4, 8, 12],stddev=0.1))\n",
    "B_Conv3 = tf.Variable(tf.constant(0.1, shape=[12]))\n",
    "\n",
    "W_Conv4 = tf.Variable(tf.truncated_normal([7*7*12, 200],stddev=0.1))\n",
    "B_Conv4 = tf.Variable(tf.constant(0.1, shape=[200]))\n",
    "\n",
    "W_Conv5 = tf.Variable(tf.truncated_normal([200, 10],stddev=0.1))\n",
    "B_Conv5 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "# Shape of input X, [batch, in_height, in_width, in_channels] ==> X = [batch_size,28 ,28, 1]\n",
    "# Shape of filter / kernel, [filter_height, filter_width, in_channels, out_channels] ==> W = [5, 5, 1, 32]\n",
    "# Shape of stride,  [batch, height, width, channels] ==> stride for 4D tensor = [1, 1, 1, 1], stride for 2D tensor [height, width] \n",
    "stride = 1  # output is 28x28\n",
    "\n",
    "Y_hat_Conv1 = tf.nn.relu(tf.nn.conv2d(X_CNN, W_Conv1, strides=[1, stride, stride, 1], padding='SAME') + B_Conv1 ) # [batch, height, width, channels]\n",
    "\n",
    "stride_conv2 = 2 # output is 14x14\n",
    "Y_hat_Conv2 = tf.nn.relu(tf.nn.conv2d(Y_hat_Conv1, W_Conv2, strides=[1, stride_conv2, stride_conv2, 1], padding='SAME') + B_Conv2 )\n",
    "\n",
    "stride_conv3 = 2  # output is 7x7\n",
    "Y_hat_Conv3 = tf.nn.relu(tf.nn.conv2d(Y_hat_Conv2, W_Conv3, strides=[1, stride_conv3, stride_conv3, 1], padding='SAME') + B_Conv3 )\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "flattened_matrix_Conv3= tf.reshape(Y_hat_Conv3, [-1, 7*7*12])\n",
    "Y_hat_Conv4 = tf.nn.relu(tf.matmul(flattened_matrix_Conv3, W_Conv4) + B_Conv4)\n",
    "Y_hat_CNN = tf.nn.softmax(tf.matmul(Y_hat_Conv4, W_Conv5) + B_Conv5)\n",
    "\n",
    "\n",
    "########################################\n",
    "# define the cost function\n",
    "########################################\n",
    "\n",
    "cross_entropy_cnn = tf.nn.softmax_cross_entropy_with_logits(logits=Y_hat_CNN, labels=Y_CNN)\n",
    "cross_entropy_cnn = tf.reduce_mean(cross_entropy_cnn) * 100\n",
    "\n",
    "########################################\n",
    "# define the optmizer\n",
    "########################################\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate_cnn)\n",
    "train_step_cnn = optimizer.minimize(cross_entropy_cnn)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "not_minimum_learning_rate = True\n",
    "max_learning_rate = 0.005\n",
    "min_learning_rate = 0.0001\n",
    "applied_learning_rate = max_learning_rate\n",
    "import math\n",
    "n_epochs = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist_cnn.train.next_batch(100)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step_cnn,cross_entropy_cnn], feed_dict={X_CNN: epoch_imageX_reshapedbatch, \n",
    "                                                                             Y_CNN: epoch_labelY_batch, \n",
    "                                                                             learning_rate_cnn: applied_learning_rate}) \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if not_minimum_learning_rate:\n",
    "            applied_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / 2000)) #Pattern Recognition and Computer Vision: First Chinese Conference pg 401\n",
    "            if applied_learning_rate == min_learning_rate:\n",
    "                not_minimum_learning_rate = False         \n",
    "        #print loss after every 200 epochs\n",
    "        if epoch % 200 == 0:\n",
    "            print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y_CNN, 1), tf.argmax(Y_hat_CNN,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist_cnn.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X_CNN:epoch_imageX_reshapedTestbatch, Y_CNN: mnist_cnn.test.labels}) * 100\n",
    "    print(\" end\")\n",
    "    print(\"The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: {} % \".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Improve The Performance\n",
    "A good approach to sizing your neural networks is to implement a network that is a little too constrained, then give it a bit more degrees of freedom and add dropout to make sure it is not overfitting. This ends up with a fairly optimal network for your problem. In the above model, we set the output channel to 4 in the first convolutional layer, which means that we repeat the same filter shape (but with different weights) four times. If we assume that those filters evolve during training into shape recognisers, you can intuitively see that this might not be enough for our problem. Handwritten digits are made from more than 4 elemental shapes. So let us bump up the filter sizes a little, and also increase the number of filters in our convolutional layers from 4, 8, 12 to 6, 12, 24 and then add dropout on the fully-connected layer. The following figure shows the new architecture you should build. Please complete the following code based on the given architecture and dropout technique.\n",
    "<img src=\"figs/18-arch2.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-782f3f58394f>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "mnist_cnn = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 has a loss of  230.3008270263672\n",
      "Epoch  200 has a loss of  159.20826721191406\n",
      "Epoch  400 has a loss of  148.1158905029297\n",
      "Epoch  600 has a loss of  151.12091064453125\n",
      "Epoch  800 has a loss of  146.20285034179688\n",
      "Epoch  1000 has a loss of  150.10943603515625\n",
      "Epoch  1200 has a loss of  151.36172485351562\n",
      "Epoch  1400 has a loss of  151.14804077148438\n",
      "Epoch  1600 has a loss of  147.1650390625\n",
      "Epoch  1800 has a loss of  148.1150665283203\n",
      "Epoch  2000 has a loss of  151.1748809814453\n",
      "Epoch  2200 has a loss of  148.1139373779297\n",
      "Epoch  2400 has a loss of  150.51304626464844\n",
      "Epoch  2600 has a loss of  150.11508178710938\n",
      "Epoch  2800 has a loss of  147.56494140625\n",
      "Epoch  3000 has a loss of  147.1151885986328\n",
      "Epoch  3200 has a loss of  148.1151123046875\n",
      "Epoch  3400 has a loss of  148.11331176757812\n",
      "Epoch  3600 has a loss of  149.1130828857422\n",
      "Epoch  3800 has a loss of  148.11505126953125\n",
      "Epoch  4000 has a loss of  150.0897979736328\n",
      "Epoch  4200 has a loss of  147.12501525878906\n",
      "Epoch  4400 has a loss of  149.74224853515625\n",
      "Epoch  4600 has a loss of  146.12197875976562\n",
      "Epoch  4800 has a loss of  146.1150665283203\n",
      " \n",
      "The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: 98.15000295639038 % \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# · · · · · · · · · ·    (input data, 1-deep)                 X [batch, 28, 28, 1]\n",
    "# @ @ @ @ @ @ @ @ @ @ -- conv. layer 6x6x1=>6 stride 1        W1 [5, 5, 1, 6]        B1 [6]\n",
    "# ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                         Y1_hat [batch, 28, 28, 6]\n",
    "#   @ @ @ @ @ @ @ @   -- conv. layer 5x5x6=>12 stride 2       W2 [5, 5, 6, 12]        B2 [12]\n",
    "#   ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                           Y2_hat [batch, 14, 14, 12]\n",
    "#     @ @ @ @ @ @     -- conv. layer 4x4x12=>24 stride 2      W3 [4, 4, 12, 24]       B3 [24]\n",
    "#     ∶∶∶∶∶∶∶∶∶∶∶                                             Y3_hat [batch, 7, 7, 24] => reshaped to YY [batch, 7*7*24]\n",
    "#      \\x/x\\x\\x/ ✞    -- fully connected layer (relu+dropout) W4 [7*7*24, 200]       B4 [200]\n",
    "#       · · · ·                                               Y4_hat [batch, 200]\n",
    "#       \\x/x\\x/       -- fully connected layer (softmax)      W5 [200, 10]           B5 [10]\n",
    "#        · · ·                                                Y_hat [batch, 10]\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "pkeep = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "# three convolutional layers with their channel counts, and a fully connected layer \n",
    "# (the last layer has 10 softmax neurons)\n",
    "# the output depth of first three convolutional layers, are 6, 12, 24, and the size of fully connected\n",
    "# layer is 200\n",
    "W_Conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 6],stddev=0.1))\n",
    "B_Conv1 = tf.Variable(tf.constant(0.1, shape=[6]))\n",
    "\n",
    "W_Conv2 = tf.Variable(tf.truncated_normal([5, 5, 6, 12],stddev=0.1))\n",
    "B_Conv2 = tf.Variable(tf.constant(0.1, shape=[12]))\n",
    "\n",
    "W_Conv3 = tf.Variable(tf.truncated_normal([4, 4, 12, 24],stddev=0.1))\n",
    "B_Conv3 = tf.Variable(tf.constant(0.1, shape=[24]))\n",
    "\n",
    "W_Conv4 = tf.Variable(tf.truncated_normal([7*7*24, 200],stddev=0.1))\n",
    "B_Conv4 = tf.Variable(tf.constant(0.1, shape=[200]))\n",
    "\n",
    "W_Conv5 = tf.Variable(tf.truncated_normal([200, 10],stddev=0.1))\n",
    "B_Conv5 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "stride = 1  # output is 28x28\n",
    "\n",
    "Y_hat_Conv1 = tf.nn.relu(tf.nn.conv2d(X, W_Conv1, strides=[1, stride, stride, 1], padding='SAME') + B_Conv1 ) # [batch, height, width, channels]\n",
    "\n",
    "stride_conv2 = 2 # output is 14x14\n",
    "Y_hat_Conv2 = tf.nn.relu(tf.nn.conv2d(Y_hat_Conv1, W_Conv2, strides=[1, stride_conv2, stride_conv2, 1], padding='SAME') + B_Conv2 )\n",
    "\n",
    "stride_conv3 = 2  # output is 7x7\n",
    "Y_hat_Conv3 = tf.nn.relu(tf.nn.conv2d(Y_hat_Conv2, W_Conv3, strides=[1, stride_conv3, stride_conv3, 1], padding='SAME') + B_Conv3 )\n",
    "\n",
    "\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "flattened_matrix_Conv3 = tf.reshape(Y_hat_Conv3, [-1, 7*7*24])\n",
    "Y_hat_Conv4 = tf.nn.relu(tf.matmul(flattened_matrix_Conv3, W_Conv4) + B_Conv4)\n",
    "Y_hat_Conv4_dropout = tf.nn.dropout(Y_hat_Conv4, keep_prob=pkeep)\n",
    "\n",
    "Y_hat_CNN2 = tf.nn.softmax(tf.matmul(Y_hat_Conv4_dropout, W_Conv5) + B_Conv5)\n",
    "\n",
    "########################################\n",
    "# define the Loss function\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y_hat_CNN2, labels=Y )\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "########################################\n",
    "# traini the model\n",
    "########################################\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "not_minimum_learning_rate = True\n",
    "max_learning_rate = 0.005\n",
    "min_learning_rate = 0.0001\n",
    "dropout_rate = 0.7\n",
    "applied_learning_rate = max_learning_rate\n",
    "import math\n",
    "n_epochs = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist_cnn.train.next_batch(100)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step,cross_entropy], feed_dict={X: epoch_imageX_reshapedbatch, \n",
    "                                                                          Y: epoch_labelY_batch, \n",
    "                                                                          learning_rate: applied_learning_rate,\n",
    "                                                                          pkeep:dropout_rate}) \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if not_minimum_learning_rate:\n",
    "            applied_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / 2000)) #Pattern Recognition and Computer Vision: First Chinese Conference pg 401\n",
    "            if applied_learning_rate == min_learning_rate:\n",
    "                not_minimum_learning_rate = False         \n",
    "        #print loss after every 200 epochs\n",
    "        if epoch % 200 == 0:\n",
    "            print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_hat_CNN2,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist_cnn.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X:epoch_imageX_reshapedTestbatch, Y: mnist_cnn.test.labels, pkeep:dropout_rate}) * 100\n",
    "    print(\" \")\n",
    "    print(\"The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: {} % \".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Tensorflow Layers Module\n",
    "The TensorFlow **layers** `tf.layers` module provides a high-level API that makes it easy to construct a neural network. It provides methods that facilitate: (i) the creation of dense (fully connected) layers and convolutional layers, (ii) adding activation functions, and (iii) applying dropout regularization. In this section use the module `tf.layers` to build the network you made in section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-f36e79a1a625>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "mnist = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 has a loss of  232.1500244140625\n",
      "Epoch  200 has a loss of  13.473519325256348\n",
      "Epoch  400 has a loss of  9.194537162780762\n",
      "Epoch  600 has a loss of  1.3236289024353027\n",
      "Epoch  800 has a loss of  0.9247559309005737\n",
      "Epoch  1000 has a loss of  1.562678575515747\n",
      "Epoch  1200 has a loss of  3.673323154449463\n",
      "Epoch  1400 has a loss of  0.5194148421287537\n",
      "Epoch  1600 has a loss of  5.202116966247559\n",
      "Epoch  1800 has a loss of  6.246393203735352\n",
      "Epoch  2000 has a loss of  0.39227771759033203\n",
      "Epoch  2200 has a loss of  0.8949481248855591\n",
      "Epoch  2400 has a loss of  0.7956770062446594\n",
      "Epoch  2600 has a loss of  11.737939834594727\n",
      "Epoch  2800 has a loss of  1.3762552738189697\n",
      "Epoch  3000 has a loss of  1.116493821144104\n",
      "Epoch  3200 has a loss of  2.229557514190674\n",
      "Epoch  3400 has a loss of  2.3982293605804443\n",
      "Epoch  3600 has a loss of  0.43825939297676086\n",
      "Epoch  3800 has a loss of  0.0859948992729187\n",
      "Epoch  4000 has a loss of  0.01970566250383854\n",
      "Epoch  4200 has a loss of  0.6534491777420044\n",
      "Epoch  4400 has a loss of  0.016693001613020897\n",
      "Epoch  4600 has a loss of  0.04560067877173424\n",
      "Epoch  4800 has a loss of  0.06011369824409485\n",
      " \n",
      "The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: 99.01000261306763 % \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "X_LM = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_LM= tf.placeholder(tf.float32, shape=(None, 10))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "pkeep = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "\n",
    "########################################\n",
    "# Create the layers\n",
    "########################################\n",
    "# Computes features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "Y_hat_conv1 = tf.layers.conv2d( inputs=X_LM, filters=6, kernel_size=[5, 5], strides=1,padding=\"same\",\n",
    "                               bias_initializer= tf.constant_initializer(0.1), activation=tf.nn.relu)\n",
    "\n",
    "Y_hat_conv2 = tf.layers.conv2d( inputs=Y_hat_conv1, filters=12, kernel_size=[5, 5], strides=2, padding=\"same\",\n",
    "                               bias_initializer= tf.constant_initializer(0.1), activation=tf.nn.relu)\n",
    "\n",
    "Y_hat_conv3 = tf.layers.conv2d( inputs=Y_hat_conv2, filters=24, kernel_size=[4, 4], strides=2, padding=\"same\",\n",
    "                               bias_initializer= tf.constant_initializer(0.1), activation=tf.nn.relu)\n",
    "\n",
    "Y_hat_conv4 = tf.layers.dense(inputs=tf.reshape(Y_hat_conv3, [-1, 7 * 7 * 24]), units=200, activation=tf.nn.relu,\n",
    "                              bias_initializer=tf.constant_initializer(0.1))\n",
    "Y_hat_conv4_dropout = tf.layers.dropout(inputs=Y_hat_conv4, rate=0.75)\n",
    "\n",
    "Y_hat_conv5 = tf.layers.dense(inputs=Y_hat_conv4_dropout, units=10,\n",
    "                         bias_initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "########################################\n",
    "# define the Loss function\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y_hat_conv5, labels=Y_LM)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "########################################\n",
    "# train the model\n",
    "########################################\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################  \n",
    "init = tf.global_variables_initializer()\n",
    "not_minimum_learning_rate = True\n",
    "max_learning_rate = 0.005\n",
    "min_learning_rate = 0.0001\n",
    "applied_learning_rate = max_learning_rate\n",
    "import math\n",
    "n_epochs = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist.train.next_batch(100)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step,cross_entropy], feed_dict={X_LM: epoch_imageX_reshapedbatch, \n",
    "                                                                          Y_LM: epoch_labelY_batch, \n",
    "                                                                          learning_rate: applied_learning_rate}) \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if not_minimum_learning_rate:\n",
    "            applied_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / 2000)) #Pattern Recognition and Computer Vision: First Chinese Conference pg 401\n",
    "            if applied_learning_rate == min_learning_rate:\n",
    "                not_minimum_learning_rate = False         \n",
    "        #print loss after every 200 epochs\n",
    "        if epoch % 200 == 0:\n",
    "            print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y_LM, 1), tf.argmax(Y_hat_conv5,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X_LM:epoch_imageX_reshapedTestbatch, Y_LM: mnist.test.labels}) * 100\n",
    "    print(\" \")\n",
    "    print(\"The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: {} % \".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Keras\n",
    "Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and production. `tf.keras` is TensorFlow's implementation of the Keras API specification. To work with Keras, you need to import `tf.keras` as part of your TensorFlow program setup.\n",
    "```\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "```\n",
    "#### Build a model\n",
    "In Keras, you assemble **layers** to build a model, i.e., a graph of layers. The most common type of model is a stack of layers: the `tf.keras.Sequential` model. For example, the following code builds a simple, fully-connected network (i.e., multi-layer perceptron):\n",
    "```\n",
    "model = tf.keras.Sequential()\n",
    "# adds a densely-connected layer with 64 units to the model:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# add another\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "There are many `tf.keras.layers` available with some common constructor parameters:\n",
    "* `activation`: set the activation function for the layer, which is specified by the name of a built-in function or as a callable object.\n",
    "* `kernel_initializer` and `bias_initializer`: the initialization schemes that create the layer's weights (weight and bias).\n",
    "* `kernel_regularizer` and `bias_regularizer`: the regularization schemes that apply the layer's weights (weight and bias), such as L1 or L2 regularization.\n",
    "\n",
    "#### Train and evaluate\n",
    "After you construct a model, you can configure its learning process by calling the `compile` method:\n",
    "```\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "The method `tf.keras.Model.compile` takes three important arguments:\n",
    "* `optimizer`: it specifies the training procedure, e.g., `tf.train.AdamOptimizer` and `tf.train.GradientDescentOptimizer`.\n",
    "* `loss`: the cost function to minimize during optimization, e.g., mean square error (mse), categorical_crossentropy, and binary_crossentropy.\n",
    "* `metrics`: used to monitor training, e.g., `accuracy`.\n",
    "\n",
    "The next step after confiuring the model is to train it by calling the `model.fit` method and giving it training data as its input. After training the model you can call `tf.keras.Model.evaluate` and `tf.keras.Model.predict` methods to evaluate the inference-mode loss and metrics for the data provided or predict the output of the last layer in inference for the data provided, respectively.\n",
    "\n",
    "You can read more about Keras [here](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "In this task, please use Keras to rebuild the network you made in section 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm unable to run this section of the code to get an output. I have conducted an online research  and as it turns outthe problem is cause by different functions in different version keras. I experienced memory issues and hardware problems in HopWorks platform, I have been using a cognitiveclass.ai plaform to run my code therefore I do not have control over which version of keras to install and use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'partition_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-355a8dc62ff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mbias_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                         padding='same', activation=\"relu\", input_shape=(28, 28, 1)))\n\u001b[0m\u001b[1;32m     41\u001b[0m model.add(layers.Conv2D(filters=12, kernel_size=5, strides=2, \n\u001b[1;32m     42\u001b[0m                         \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \"\"\"\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# Actually call the layer (optionally building it).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_uses_inputs_arg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    142\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                                     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                     dtype=self.dtype)\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       self.bias = self.add_variable(name='bias',\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             partitioner=partitioner)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    434\u001b[0m     new_variable = getter(\n\u001b[1;32m    435\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         use_resource=use_resource)\n\u001b[0m\u001b[1;32m    787\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_eager_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;31m# In eager mode we do not want to keep default references to Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)\u001b[0m\n\u001b[1;32m   2218\u001b[0m                          \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                          \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m                          use_resource=use_resource)\n\u001b[0m\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   2208\u001b[0m              \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m              use_resource=None):\n\u001b[0;32m-> 2210\u001b[0;31m   \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2191\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m         \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m         constraint=constraint)\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m               self._initial_value = ops.convert_to_tensor(\n\u001b[0;32m--> 343\u001b[0;31m                   initial_value(), name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[1;32m    344\u001b[0m               shape = (self._initial_value.get_shape()\n\u001b[1;32m    345\u001b[0m                        if validate_shape else tensor_shape.unknown_shape())\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    768\u001b[0m           \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n\u001b[0;32m--> 770\u001b[0;31m             shape.as_list(), dtype=dtype, partition_info=partition_info)\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'partition_info'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Dropout, Dense, Flatten\n",
    "from tensorflow.python.keras import layers\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import math\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "# Split mnist data into train and test sets\n",
    "(X_training_data, Y_training_labels), (X_testing_data, Y_testing_labels) = mnist.load_data()\n",
    "\n",
    "# reshape data to fit model\n",
    "X_training_data = X_training_data.reshape(-1, 28, 28, 1)\n",
    "X_testing_data = X_testing_data.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# one-hot encoding for the target column e.g a column with digit 5 will be replaced with binary representation of 1 then the rest of colums with 0s\n",
    "Y_one_hot_training_labels = to_categorical(Y_training_labels)\n",
    "Y_one_hot_testing_labels = to_categorical(Y_testing_labels)\n",
    "\n",
    "########################################\n",
    "# Build the network model |Functional model\n",
    "########################################\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(filters=6, kernel_size=5, strides=1, \n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), \n",
    "                        bias_initializer = keras.initializers.Constant(value=0.1),\n",
    "                        padding='same', activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(layers.Conv2D(filters=12, kernel_size=5, strides=2, \n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), \n",
    "                        bias_initializer = keras.initializers.Constant(value=0.1),\n",
    "                        padding='same', activation=\"relu\", input_shape=(28, 28, 6)))\n",
    "model.add(layers.Conv2D(filters=24, kernel_size=4, strides=2, \n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), \n",
    "                        bias_initializer = keras.initializers.Constant(value=0.1),\n",
    "                        padding='same', activation=\"relu\", input_shape=(14, 14, 12)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully-connected layer with 200 units and ReLU activation\n",
    "model.add(layers.Dense(200, activation=\"relu\", kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None)\n",
    "                       ,bias_initializer= tf.keras.initializers.Constant(value=0.1)))\n",
    "model.add(Dropout(0.75))\n",
    "\n",
    "# Output layer with 10 units and a softmax activation\n",
    "model.add(tf.keras.layers.Dense(10, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                                activation=\"softmax\", bias_initializer = tf.keras.initializers.Constant(value=0.1)))\n",
    "\n",
    "########################################\n",
    "# Compile the model\n",
    "########################################\n",
    "\n",
    "# learning rate schedule\n",
    "def lr_exponential_decay(epoch, lr):\n",
    "    # step_decay:lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    # exponential decay: 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔_𝑟𝑎𝑡𝑒=𝑙𝑟_𝑚𝑖𝑛+(𝑙𝑟_𝑚𝑎𝑥−𝑙𝑟_𝑚𝑖𝑛)∗𝑒−𝑖2000, where 𝑖 is the iteration number.\n",
    "    max_learning_rate = 0.005\n",
    "    min_learning_rate = 0.0001\n",
    "    epochs_drop = 2\n",
    "    lr = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / 2000))\n",
    "    return lr\n",
    "\n",
    "# learning schedule callback\n",
    "callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_exponential_decay, verbose=0)]\n",
    "    \n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "########################################\n",
    "# train and execute the model\n",
    "########################################\n",
    "# model.fit(X_training_data, Y_training_labels, validation_data=(X_testing_data , Y_testing_labels), callbacks=callbacks, epochs=5, batch_size=100,verbose=1)\n",
    "model.fit(X_training_data, Y_one_hot_training_labels, validation_data=(X_testing_data , Y_one_hot_testing_labels),epochs=5, batch_size=100,verbose=1)\n",
    "########################################\n",
    "# execute the model\n",
    "########################################  \n",
    "model.predict(X_testing_data)\n",
    "########################################\n",
    "# execute the model\n",
    "########################################  \n",
    "model.predict(Y_testing_labels)\n",
    "\n",
    "#Y_one_hot_training_labels = to_categorical(Y_training_labels)\n",
    "#Y_one_hot_testing_labels = to_categorical(Y_testing_labels)\n",
    "#print(\" \")\n",
    "#print(\"The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: {} % \".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Implement LeNet-5\n",
    "In this section, you should implement **LeNet-5** either using Tensorflow or Keras. Please take a look at its [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) before starting to implement it.\n",
    "The LeNet-5 architecture is perhaps the most widely known CNN architecture. It was created by Yann LeCun in 1998 and widely used for handwritten digit recognition (MNIST). It is composed of the layers shown in the following table.\n",
    "<img src=\"figs/19-letnet5.png\" style=\"width: 600px;\"/>\n",
    "There are a few extra details to be noted:\n",
    "* MNIST images are 28×28 pixels, but they are zero-padded to 32×32 pixels and normalized before being fed to the network. The rest of the network does not use any padding, which is why the size keeps shrinking as the image progresses through the network.\n",
    "* The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coefficient and adds a learnable bias term, then finally applies the activation function.\n",
    "* Most neurons in layer C3 maps are connected to neurons in only three or four S2 maps (instead of all six S2 maps). See table 1 in the [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) for details.\n",
    "* The output layer is a bit special: instead of computing the dot product of the inputs and the weight vector, each neuron outputs the square of the Euclidian distance between its input vector and its weight vector. Each output measures how much the image belongs to a particular digit class. The cross-entropy cost function is now preferred, as it penalizes bad predictions much more, producing larger gradients and thus converging faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 1000 values, but the requested shape has 10\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape)]]\n\nCaused by op 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape', defined at:\n  File \"/home/jupyterlab/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jupyterlab/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/jupyterlab/conda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/jupyterlab/conda/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/jupyterlab/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6c9b442bbe2b>\", line 59, in <module>\n    train_step = optimizer.minimize(cross_entropy)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in minimize\n    grad_loss=grad_loss)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 526, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 385, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 521, in _ReshapeGrad\n    return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'softmax_cross_entropy_with_logits_sg/Reshape', defined at:\n  File \"/home/jupyterlab/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 25 identical lines from previous traceback]\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6c9b442bbe2b>\", line 52, in <module>\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=Y_LN5)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1959, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1866, in softmax_cross_entropy_with_logits_v2\n    precise_logits = _flatten_outer_dims(precise_logits)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 1000 values, but the requested shape has 10\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 1000 values, but the requested shape has 10\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6c9b442bbe2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         _, loss = sess.run([train_step,cross_entropy], feed_dict={X_LN5: epoch_imageX_reshapedbatch, \n\u001b[1;32m     79\u001b[0m                                                                           \u001b[0mY_LN5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_labelY_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                                                                           learning_rate_ln5: applied_learning_rate}) \n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_minimum_learning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 1000 values, but the requested shape has 10\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape)]]\n\nCaused by op 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape', defined at:\n  File \"/home/jupyterlab/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jupyterlab/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/jupyterlab/conda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/jupyterlab/conda/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/jupyterlab/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6c9b442bbe2b>\", line 59, in <module>\n    train_step = optimizer.minimize(cross_entropy)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in minimize\n    grad_loss=grad_loss)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 526, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 385, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 521, in _ReshapeGrad\n    return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'softmax_cross_entropy_with_logits_sg/Reshape', defined at:\n  File \"/home/jupyterlab/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 25 identical lines from previous traceback]\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6c9b442bbe2b>\", line 52, in <module>\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=Y_LN5)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1959, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1866, in softmax_cross_entropy_with_logits_v2\n    precise_logits = _flatten_outer_dims(precise_logits)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/jupyterlab/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 1000 values, but the requested shape has 10\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape)]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build the LetNet-5 model, and test it on MNIST\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "mnist = mnist_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "########################################\n",
    "# define variables and placeholders\n",
    "########################################\n",
    "X_LN5 = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_LN5 = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "learning_rate_ln5 = tf.placeholder(tf.float32)\n",
    "pkeep_ln5 = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "\n",
    "########################################\n",
    "# Create the layers\n",
    "########################################\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "Y_hat_conv1 = tf.layers.conv2d(inputs=X_LN5, filters=6, kernel_size=[5, 5], strides=1,padding=\"same\",\n",
    "                               bias_initializer= tf.constant_initializer(0.1), activation=tf.nn.tanh)\n",
    "S2 = tf.nn.avg_pool(Y_hat_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # avg_pool_2x2\n",
    "\n",
    "Y_hat_conv3 = tf.layers.conv2d(inputs=S2, filters=16, kernel_size=[5, 5], strides=2, padding=\"same\",\n",
    "                               bias_initializer= tf.constant_initializer(0.1), activation=tf.nn.tanh)\n",
    "\n",
    "S4 = tf.nn.avg_pool(Y_hat_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # avg_pool_2x2\n",
    "\n",
    "Y_hat_conv5 = tf.layers.conv2d(inputs=S4, filters=120, kernel_size=[5, 5], strides=2, padding=\"same\",\n",
    "                               bias_initializer= tf.constant_initializer(0.1), activation=tf.nn.tanh)\n",
    "\n",
    "F6 = tf.layers.dense(inputs=tf.reshape(Y_hat_conv5, [-1, 20*20*120]), units=84, activation=tf.nn.tanh,\n",
    "                              bias_initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "F6_dropout = tf.layers.dropout(inputs=F6, rate=0.75)\n",
    "\n",
    "out  = tf.layers.dense(inputs=F6_dropout, units=10,\n",
    "                         bias_initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "########################################\n",
    "# define the Loss function\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=Y_LN5)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "########################################\n",
    "# train the model\n",
    "########################################\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_ln5)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################  \n",
    "init = tf.global_variables_initializer()\n",
    "not_minimum_learning_rate = True\n",
    "max_learning_rate = 0.005\n",
    "min_learning_rate = 0.0001\n",
    "applied_learning_rate = max_learning_rate\n",
    "import math\n",
    "n_epochs = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #For every iteration i\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_imageX_batch, epoch_labelY_batch = mnist.train.next_batch(100)\n",
    "        epoch_imageX_reshapedbatch = np.reshape(epoch_imageX_batch, [-1,28,28,1])\n",
    "        _, loss = sess.run([train_step,cross_entropy], feed_dict={X_LN5: epoch_imageX_reshapedbatch, \n",
    "                                                                          Y_LN5: epoch_labelY_batch, \n",
    "                                                                          learning_rate_ln5: applied_learning_rate}) \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if not_minimum_learning_rate:\n",
    "            applied_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch / 2000)) #Pattern Recognition and Computer Vision: First Chinese Conference pg 401\n",
    "            if applied_learning_rate == min_learning_rate:\n",
    "                not_minimum_learning_rate = False         \n",
    "        #print loss after every 200 epochs\n",
    "        if epoch % 200 == 0:\n",
    "            print ('Epoch ',epoch, 'has a loss of ', epoch_loss)\n",
    "            \n",
    "    predictions = tf.equal(tf.argmax(Y_LN5, 1), tf.argmax(out,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    epoch_imageX_reshapedTestbatch = np.reshape(mnist.test.images, [-1,28,28,1])\n",
    "    acc = accuracy.eval(session=sess,feed_dict={X_LN5:epoch_imageX_reshapedTestbatch, Y_LN5: mnist.test.labels}) * 100\n",
    "    print(\" \")\n",
    "    print(\"The Accuracy for an fine tuned model in Five-Layer CNN Neural Network model is: {} % \".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 11. Implement AlexNet\n",
    "In the last section, you should implement **AlexNet** either using Tensorflow or Keras. Again, please take a look at its [paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) before start to implement it.\n",
    "The AlexNet CNN architecture won the [ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2012/) in 2012 by a large margin. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. The following table presents this architecture.\n",
    "<img src=\"figs/20-alexnet.png\" style=\"width: 600px;\"/>\n",
    "To train the model, we need a big dataset, however, in this assignment you are going to to assign the pretrained weights to your model, using `tf.Variable.assign`. You can download the pretrained weights from [bvlc_alexnet.npy](https://www.cs.toronto.edu/~guerzhoy/tf_alexnet/bvlc_alexnet.npy). This file is a NumPy array file created by the python. After you read this file, you will receive a python dictionary with a <key, value> pair for each layer. Each key is one of the layers names, e.g., `conv1`, and each value is a list of two values: (1) weights, and (2) biases of that layer. Part of the function to load the weights and biases to your model is given, and you need to complete it.\n",
    "\n",
    "Here is what you see if you read and print the shape of each layer from the file:\n",
    "```\n",
    "weight_dic = np.load(\"bvlc_alexnet.npy\", encoding=\"bytes\").item()\n",
    "for layer in weights_dic:\n",
    "    print(\"-\" * 20)\n",
    "    print(layer)\n",
    "    for wb in weights_dic[layer]:\n",
    "        print(wb.shape)\n",
    "\n",
    "#--------------------\n",
    "# fc8\n",
    "# (4096, 1000) # weights\n",
    "# (1000,) # bias\n",
    "#--------------------\n",
    "# fc7\n",
    "# (4096, 4096) # weights\n",
    "# (4096,) # bias\n",
    "#--------------------\n",
    "# fc6\n",
    "# (9216, 4096) # weights\n",
    "# (4096,) # bias\n",
    "#--------------------\n",
    "# conv5\n",
    "# (3, 3, 192, 256) # weights\n",
    "# (256,) # bias\n",
    "#--------------------\n",
    "# conv4\n",
    "# (3, 3, 192, 384) # weights\n",
    "# (384,) # bias\n",
    "#--------------------\n",
    "# conv3\n",
    "# (3, 3, 256, 384) # weights\n",
    "# (384,) # bias\n",
    "#--------------------\n",
    "# conv2\n",
    "# (5, 5, 48, 256) # weights\n",
    "# (256,) # bias\n",
    "#--------------------\n",
    "# conv1\n",
    "# (11, 11, 3, 96) # weights\n",
    "# (96,) # bias\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 1716 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "2018-12-24 04:50:57,091 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-12-24 04:50:57,835 INFO  RMProxy: Connecting to ResourceManager at /10.0.104.193:8032\n",
      "2018-12-24 04:50:58,270 INFO  Client: Requesting a new application from cluster with 30 NodeManagers\n",
      "2018-12-24 04:50:58,403 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (216000 MB per container)\n",
      "2018-12-24 04:50:58,414 INFO  Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "2018-12-24 04:50:58,415 INFO  Client: Setting up container launch context for our AM\n",
      "2018-12-24 04:50:58,432 INFO  Client: Setting up the launch environment for our AM container\n",
      "2018-12-24 04:50:58,445 INFO  Client: Preparing resources for our AM container\n",
      "2018-12-24 04:51:00,145 WARN  Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2018-12-24 04:51:04,591 INFO  Client: Uploading resource file:/tmp/spark-6831429d-1892-4f9b-b8b7-56f5a95be4f8/__spark_libs__5308165938438045151.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/__spark_libs__5308165938438045151.zip\n",
      "2018-12-24 04:51:05,900 INFO  Client: Uploading resource file:/srv/hops/livy/rsc-jars/livy-api-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-api-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:06,982 INFO  Client: Uploading resource file:/srv/hops/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/netty-all-4.1.17.Final.jar\n",
      "2018-12-24 04:51:07,165 INFO  Client: Uploading resource file:/srv/hops/livy/rsc-jars/livy-rsc-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-rsc-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:07,335 INFO  Client: Uploading resource file:/srv/hops/livy/repl_2.11-jars/livy-repl_2.11-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-repl_2.11-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:07,820 INFO  Client: Uploading resource file:/srv/hops/livy/repl_2.11-jars/livy-core_2.11-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-core_2.11-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:07,965 INFO  Client: Uploading resource file:/srv/hops/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/commons-codec-1.9.jar\n",
      "2018-12-24 04:51:08,049 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2018-12-24 04:51:08,073 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/cacerts.jks#domain_ca_truststore\n",
      "2018-12-24 04:51:08,080 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hops-util-0.6.0.jar\n",
      "2018-12-24 04:51:08,085 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/glassfish/kafkacerts/ImageRecognition__rakama00/ImageRecognition__rakama00__kstore.jks#k_certificate\n",
      "2018-12-24 04:51:08,102 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/glassfish/kafkacerts/ImageRecognition__rakama00/ImageRecognition__rakama00__tstore.jks#t_certificate\n",
      "2018-12-24 04:51:08,108 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/glassfish/kafkacerts/ImageRecognition__rakama00/ImageRecognition__rakama00__cert.key#material_passwd\n",
      "2018-12-24 04:51:08,114 INFO  Client: Uploading resource file:/srv/hops/spark/R/lib/sparkr.zip#sparkr -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/sparkr.zip\n",
      "2018-12-24 04:51:08,288 INFO  Client: Uploading resource file:/srv/hops/spark/python/lib/pyspark.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/pyspark.zip\n",
      "2018-12-24 04:51:08,371 INFO  Client: Uploading resource file:/srv/hops/spark/python/lib/py4j-0.10.7-src.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/py4j-0.10.7-src.zip\n",
      "2018-12-24 04:51:09,792 INFO  Client: Uploading resource file:/tmp/spark-6831429d-1892-4f9b-b8b7-56f5a95be4f8/__spark_conf__3706416092524814140.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/__spark_conf__.zip\n",
      "2018-12-24 04:51:09,922 INFO  SecurityManager: Changing view acls to: livy,ImageRecognition__rakama00\n",
      "2018-12-24 04:51:09,923 INFO  SecurityManager: Changing modify acls to: livy,ImageRecognition__rakama00\n",
      "2018-12-24 04:51:09,923 INFO  SecurityManager: Changing view acls groups to: \n",
      "2018-12-24 04:51:09,924 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2018-12-24 04:51:09,925 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, ImageRecognition__rakama00); groups with view permissions: Set(); users  with modify permissions: Set(livy, ImageRecognition__rakama00); groups with modify permissions: Set()\n",
      "2018-12-24 04:51:09,944 INFO  Client: Submitting application application_1544690131655_0382 to ResourceManager\n",
      "2018-12-24 04:51:09,988 INFO  YarnClientImpl: Submitted application application_1544690131655_0382\n",
      "2018-12-24 04:51:09,990 INFO  Client: Application report for application_1544690131655_0382 (state: ACCEPTED)\n",
      "2018-12-24 04:51:09,993 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Mon Dec 24 04:51:09 +0100 2018] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1545623469965\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop33:8088/proxy/application_1544690131655_0382/\n",
      "\t user: ImageRecognition__rakama00\n",
      "2018-12-24 04:51:09,996 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2018-12-24 04:51:09,997 INFO  ShutdownHookManager: Deleting directory /tmp/spark-dfb6880f-e70f-42d5-b851-7adf35b0549a\n",
      "2018-12-24 04:51:09,999 INFO  ShutdownHookManager: Deleting directory /tmp/spark-6831429d-1892-4f9b-b8b7-56f5a95be4f8\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "# build the AlexNet model\n",
    "<FILL IN> :)\n",
    "\n",
    "# load inital weights and biases to the model\n",
    "def load_initial_weights(self, session):\n",
    "    # load the weights into memory\n",
    "    weights_dic = np.load('bvlc_alexnet.npy', encoding='bytes').item()\n",
    "\n",
    "    # loop over all layer names stored in the weights dict\n",
    "    for layer in weights_dict:\n",
    "        with tf.variable_scope(layer, reuse=True):\n",
    "            # loop over list of weights/biases and assign them to their corresponding tf variable\n",
    "            for wb in weights_dict[layer]:\n",
    "                # biases\n",
    "                if len(wb.shape) == 1:\n",
    "                    bias = tf.get_variable(<FILL IN>)\n",
    "                    session.run(bias.assign(wb))\n",
    "                # weights\n",
    "                else:\n",
    "                    weight = tf.get_variable(<FILL IN>)\n",
    "                    session.run(weight.assign(wb))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model\n",
    "After building the AlexNet model, you can test it on different images and present the accuracy of the model. To do so, first you need to use **OpenCV** library to make the images ready to give as input to the model. OpenCV is a library used for image processing. Below you can see how to read an image file and pre-process it using OpenCV to give it to the model. However, you need to complete the code and test the accuracy of your model. The teset images (shown below) are available in the `test_images` folder.\n",
    "<table width=\"100%\">\n",
    "<tr>\n",
    "<td><img src=\"test_images/test_image1.jpg\" style=\"width:200px;\"></td>\n",
    "<td><p align=\"center\"><img src=\"test_images/test_image2.jpg\" style=\"width:200px;\"></td>\n",
    "<td align=\"right\"><img src=\"test_images/test_image3.jpg\" style=\"width:200px;\"></td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 1716 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "2018-12-24 04:50:57,091 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-12-24 04:50:57,835 INFO  RMProxy: Connecting to ResourceManager at /10.0.104.193:8032\n",
      "2018-12-24 04:50:58,270 INFO  Client: Requesting a new application from cluster with 30 NodeManagers\n",
      "2018-12-24 04:50:58,403 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (216000 MB per container)\n",
      "2018-12-24 04:50:58,414 INFO  Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "2018-12-24 04:50:58,415 INFO  Client: Setting up container launch context for our AM\n",
      "2018-12-24 04:50:58,432 INFO  Client: Setting up the launch environment for our AM container\n",
      "2018-12-24 04:50:58,445 INFO  Client: Preparing resources for our AM container\n",
      "2018-12-24 04:51:00,145 WARN  Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2018-12-24 04:51:04,591 INFO  Client: Uploading resource file:/tmp/spark-6831429d-1892-4f9b-b8b7-56f5a95be4f8/__spark_libs__5308165938438045151.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/__spark_libs__5308165938438045151.zip\n",
      "2018-12-24 04:51:05,900 INFO  Client: Uploading resource file:/srv/hops/livy/rsc-jars/livy-api-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-api-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:06,982 INFO  Client: Uploading resource file:/srv/hops/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/netty-all-4.1.17.Final.jar\n",
      "2018-12-24 04:51:07,165 INFO  Client: Uploading resource file:/srv/hops/livy/rsc-jars/livy-rsc-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-rsc-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:07,335 INFO  Client: Uploading resource file:/srv/hops/livy/repl_2.11-jars/livy-repl_2.11-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-repl_2.11-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:07,820 INFO  Client: Uploading resource file:/srv/hops/livy/repl_2.11-jars/livy-core_2.11-0.5.1-incubating-SNAPSHOT.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/livy-core_2.11-0.5.1-incubating-SNAPSHOT.jar\n",
      "2018-12-24 04:51:07,965 INFO  Client: Uploading resource file:/srv/hops/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/commons-codec-1.9.jar\n",
      "2018-12-24 04:51:08,049 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2018-12-24 04:51:08,073 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/cacerts.jks#domain_ca_truststore\n",
      "2018-12-24 04:51:08,080 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hops-util-0.6.0.jar\n",
      "2018-12-24 04:51:08,085 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/glassfish/kafkacerts/ImageRecognition__rakama00/ImageRecognition__rakama00__kstore.jks#k_certificate\n",
      "2018-12-24 04:51:08,102 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/glassfish/kafkacerts/ImageRecognition__rakama00/ImageRecognition__rakama00__tstore.jks#t_certificate\n",
      "2018-12-24 04:51:08,108 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/glassfish/kafkacerts/ImageRecognition__rakama00/ImageRecognition__rakama00__cert.key#material_passwd\n",
      "2018-12-24 04:51:08,114 INFO  Client: Uploading resource file:/srv/hops/spark/R/lib/sparkr.zip#sparkr -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/sparkr.zip\n",
      "2018-12-24 04:51:08,288 INFO  Client: Uploading resource file:/srv/hops/spark/python/lib/pyspark.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/pyspark.zip\n",
      "2018-12-24 04:51:08,371 INFO  Client: Uploading resource file:/srv/hops/spark/python/lib/py4j-0.10.7-src.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/py4j-0.10.7-src.zip\n",
      "2018-12-24 04:51:09,792 INFO  Client: Uploading resource file:/tmp/spark-6831429d-1892-4f9b-b8b7-56f5a95be4f8/__spark_conf__3706416092524814140.zip -> hdfs:/Projects/ImageRecognition/Resources/.sparkStaging/application_1544690131655_0382/__spark_conf__.zip\n",
      "2018-12-24 04:51:09,922 INFO  SecurityManager: Changing view acls to: livy,ImageRecognition__rakama00\n",
      "2018-12-24 04:51:09,923 INFO  SecurityManager: Changing modify acls to: livy,ImageRecognition__rakama00\n",
      "2018-12-24 04:51:09,923 INFO  SecurityManager: Changing view acls groups to: \n",
      "2018-12-24 04:51:09,924 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2018-12-24 04:51:09,925 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, ImageRecognition__rakama00); groups with view permissions: Set(); users  with modify permissions: Set(livy, ImageRecognition__rakama00); groups with modify permissions: Set()\n",
      "2018-12-24 04:51:09,944 INFO  Client: Submitting application application_1544690131655_0382 to ResourceManager\n",
      "2018-12-24 04:51:09,988 INFO  YarnClientImpl: Submitted application application_1544690131655_0382\n",
      "2018-12-24 04:51:09,990 INFO  Client: Application report for application_1544690131655_0382 (state: ACCEPTED)\n",
      "2018-12-24 04:51:09,993 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Mon Dec 24 04:51:09 +0100 2018] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1545623469965\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop33:8088/proxy/application_1544690131655_0382/\n",
      "\t user: ImageRecognition__rakama00\n",
      "2018-12-24 04:51:09,996 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2018-12-24 04:51:09,997 INFO  ShutdownHookManager: Deleting directory /tmp/spark-dfb6880f-e70f-42d5-b851-7adf35b0549a\n",
      "2018-12-24 04:51:09,999 INFO  ShutdownHookManager: Deleting directory /tmp/spark-6831429d-1892-4f9b-b8b7-56f5a95be4f8\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# test the AlexNet model on the given images\n",
    "\n",
    "import cv2\n",
    "\n",
    "#get list of all images\n",
    "current_dir = os.getcwd()\n",
    "image_path = os.path.join(current_dir, 'test_images')\n",
    "img_files = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
    "\n",
    "#load all images\n",
    "imgs = []\n",
    "for f in img_files:\n",
    "    imgs.append(cv2.imread(f))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    <FILL IN>\n",
    "    \n",
    "    # loop over all images\n",
    "    for i, image in enumerate(imgs):\n",
    "        # convert image to float32 and resize to (227x227)\n",
    "        img = cv2.resize(image.astype(np.float32), (227, 227))\n",
    "        \n",
    "        # subtract the ImageNet mean\n",
    "        # Mean subtraction per channel was used to center the data around zero mean for each channel (R, G, B).\n",
    "        # This typically helps the network to learn faster since gradients act uniformly for each channel.\n",
    "        imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "        img -= imagenet_mean\n",
    "        \n",
    "        # reshape as needed to feed into model\n",
    "        img = img.reshape((1, 227, 227, 3))\n",
    "        \n",
    "        <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
